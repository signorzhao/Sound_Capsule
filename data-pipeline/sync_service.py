"""
äº‘ç«¯åŒæ­¥æœåŠ¡æ¨¡å—

æä¾›æœ¬åœ°æ•°æ®åº“ä¸äº‘ç«¯ API ä¹‹é—´çš„æ•°æ®åŒæ­¥åŠŸèƒ½
"""

import sqlite3
import hashlib
import json
import logging
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
_UPLOAD_PROGRESS = {}
_UPLOAD_PROGRESS_LOCK = threading.Lock()


def _set_upload_progress(capsule_id: int, data: Dict[str, Any]) -> None:
    with _UPLOAD_PROGRESS_LOCK:
        _UPLOAD_PROGRESS[capsule_id] = {
            **data,
            'capsule_id': capsule_id,
            'updated_at': datetime.utcnow().isoformat()
        }


def _get_upload_progress(capsule_id: int) -> Optional[Dict[str, Any]]:
    with _UPLOAD_PROGRESS_LOCK:
        return _UPLOAD_PROGRESS.get(capsule_id)


def _clear_upload_progress(capsule_id: int) -> None:
    with _UPLOAD_PROGRESS_LOCK:
        _UPLOAD_PROGRESS.pop(capsule_id, None)
from pathlib import Path


class SyncService:
    """åŒæ­¥æœåŠ¡ç±»"""

    def __init__(self, db_path: str, api_base_url: str = None):
        """
        åˆå§‹åŒ–åŒæ­¥æœåŠ¡

        Args:
            db_path: æœ¬åœ°æ•°æ®åº“è·¯å¾„
            api_base_url: äº‘ç«¯ API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
        """
        self.db_path = db_path
        self.api_base_url = api_base_url or "https://api.soundcapsule.com/api/v2"

    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(
            self.db_path,
            timeout=30.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 30 ç§’ï¼Œé¿å…å¹¶å‘æ—¶çš„é”ç­‰å¾…
            check_same_thread=False  # å…è®¸å¤šçº¿ç¨‹è®¿é—®
        )
        conn.row_factory = sqlite3.Row
        return conn

    def _has_local_audio_files(self, capsule_dir: Path) -> bool:
        """æ£€æŸ¥æœ¬åœ° Audio æ–‡ä»¶å¤¹æ˜¯å¦åŒ…å«éŸ³é¢‘æ–‡ä»¶"""
        audio_dir = capsule_dir / "Audio"
        if not audio_dir.exists() or not audio_dir.is_dir():
            return False
        for entry in audio_dir.iterdir():
            if entry.is_file() and not entry.name.startswith('.'):
                return True
        return False

    def _update_asset_status_if_needed(self, capsule_id: int, current_status: Optional[str], new_status: str) -> bool:
        """ä»…åœ¨éœ€è¦æ—¶æ›´æ–° asset_status"""
        if current_status == new_status:
            return False
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE capsules
                SET asset_status = ?,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (new_status, capsule_id))
            conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            logger.warning(f"   âš ï¸  æ›´æ–° asset_status å¤±è´¥: {capsule_id} - {e}")
            conn.rollback()
            return False
        finally:
            conn.close()

    def _save_metadata_to_db(self, capsule_id: int, metadata_path: Path) -> bool:
        """
        ä» metadata.json æ–‡ä»¶è¯»å–æŠ€æœ¯å…ƒæ•°æ®å¹¶å†™å…¥ capsule_metadata è¡¨
        
        Args:
            capsule_id: èƒ¶å›Š ID
            metadata_path: metadata.json æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # ğŸ”¥ ä¿®å¤ï¼šæ”¯æŒä¸¤ç§ plugins æ ¼å¼
            # æ ¼å¼ 1 (åµŒå¥—): {"plugins": {"count": 1, "list": [...]}}
            # æ ¼å¼ 2 (æ‰å¹³): {"plugin_count": 1, "plugin_list": [...]}
            plugins = metadata.get('plugins', {})
            if isinstance(plugins, dict):
                plugin_count = plugins.get('count', metadata.get('plugin_count'))
                plugin_list = plugins.get('list', metadata.get('plugin_list', []))
            else:
                plugin_count = metadata.get('plugin_count')
                plugin_list = metadata.get('plugin_list', [])
            
            # ä» metadata.json æå–æŠ€æœ¯ä¿¡æ¯
            tech_metadata = {
                'bpm': metadata.get('bpm'),
                'duration': metadata.get('duration'),
                'sample_rate': metadata.get('sample_rate'),
                'plugin_count': plugin_count,
                'plugin_list': plugin_list,
                'has_sends': metadata.get('has_sends'),
                'has_folder_bus': metadata.get('has_folder_bus'),
                'tracks_included': metadata.get('tracks_included')
            }
            
            # è°ƒç”¨æ•°æ®åº“æ–¹æ³•ä¿å­˜
            from capsule_db import get_database
            db = get_database()
            success = db.save_capsule_metadata(capsule_id, tech_metadata)
            
            if success:
                logger.info(f"   ğŸ“Š æŠ€æœ¯å…ƒæ•°æ®å·²å†™å…¥æ•°æ®åº“")
            return success
            
        except json.JSONDecodeError as e:
            logger.warning(f"   âš ï¸  metadata.json è§£æå¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.warning(f"   âš ï¸  ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            return False

    def repair_missing_metadata(self) -> Dict[str, Any]:
        """
        ä¿®å¤ç¼ºå¤±çš„ capsule_metadata æ•°æ®
        
        æ‰«ææ‰€æœ‰èƒ¶å›Šï¼Œæ£€æŸ¥ capsule_metadata è¡¨æ˜¯å¦æœ‰å¯¹åº”è®°å½•ï¼Œ
        å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»æœ¬åœ° metadata.json æ–‡ä»¶è¯»å–å¹¶å†™å…¥æ•°æ®åº“ã€‚
        
        Returns:
            ä¿®å¤ç»“æœç»Ÿè®¡
        """
        from capsule_db import get_database
        from common import PathManager
        
        logger.info("ğŸ”§ å¼€å§‹ä¿®å¤ç¼ºå¤±çš„æŠ€æœ¯å…ƒæ•°æ®...")
        
        repaired = 0
        skipped = 0
        failed = 0
        errors = []
        
        try:
            pm = PathManager.get_instance()
            export_dir = pm.export_dir
            
            conn = self._get_connection()
            try:
                cursor = conn.cursor()
                
                # æŸ¥æ‰¾ç¼ºå¤± capsule_metadata çš„èƒ¶å›Š
                cursor.execute("""
                    SELECT c.id, c.name, c.file_path
                    FROM capsules c
                    LEFT JOIN capsule_metadata m ON c.id = m.capsule_id
                    WHERE m.capsule_id IS NULL
                """)
                missing_capsules = cursor.fetchall()
                
            finally:
                conn.close()
            
            if not missing_capsules:
                logger.info("   âœ“ æ‰€æœ‰èƒ¶å›Šéƒ½æœ‰æŠ€æœ¯å…ƒæ•°æ®ï¼Œæ— éœ€ä¿®å¤")
                return {
                    'success': True,
                    'repaired': 0,
                    'skipped': 0,
                    'failed': 0,
                    'message': 'æ‰€æœ‰èƒ¶å›Šéƒ½æœ‰æŠ€æœ¯å…ƒæ•°æ®'
                }
            
            logger.info(f"   å‘ç° {len(missing_capsules)} ä¸ªèƒ¶å›Šç¼ºå¤±æŠ€æœ¯å…ƒæ•°æ®")
            
            for capsule in missing_capsules:
                cap_id = capsule['id'] if isinstance(capsule, sqlite3.Row) else capsule[0]
                cap_name = capsule['name'] if isinstance(capsule, sqlite3.Row) else capsule[1]
                cap_file_path = capsule['file_path'] if isinstance(capsule, sqlite3.Row) else capsule[2]
                
                # æ„å»º metadata.json è·¯å¾„
                capsule_rel_path = cap_file_path or cap_name
                capsule_dir = Path(export_dir) / capsule_rel_path
                metadata_path = capsule_dir / "metadata.json"
                
                if not metadata_path.exists():
                    logger.warning(f"   âš ï¸  {cap_name}: metadata.json ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    skipped += 1
                    continue
                
                # è¯»å–å¹¶å†™å…¥æ•°æ®åº“
                success = self._save_metadata_to_db(cap_id, metadata_path)
                
                if success:
                    logger.info(f"   âœ“ {cap_name}: æŠ€æœ¯å…ƒæ•°æ®å·²ä¿®å¤")
                    repaired += 1
                else:
                    logger.error(f"   âœ— {cap_name}: ä¿®å¤å¤±è´¥")
                    failed += 1
                    errors.append(f"{cap_name}: å†™å…¥æ•°æ®åº“å¤±è´¥")
            
            logger.info(f"ğŸ”§ ä¿®å¤å®Œæˆ: æˆåŠŸ {repaired}, è·³è¿‡ {skipped}, å¤±è´¥ {failed}")
            
            return {
                'success': True,
                'repaired': repaired,
                'skipped': skipped,
                'failed': failed,
                'errors': errors
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤å¤±è´¥: {e}")
            return {
                'success': False,
                'repaired': repaired,
                'skipped': skipped,
                'failed': failed,
                'errors': [str(e)]
            }

    def _dedupe_local_capsules(self) -> None:
        """å»é‡æœ¬åœ°åŒååŒè·¯å¾„èƒ¶å›Šï¼Œä¿ç•™ä¸€æ¡è®°å½•"""
        from capsule_db import get_database

        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT name, file_path, COUNT(*) as cnt
                FROM capsules
                WHERE name IS NOT NULL AND file_path IS NOT NULL
                GROUP BY name, file_path
                HAVING cnt > 1
            """)
            groups = cursor.fetchall()
        finally:
            conn.close()

        if not groups:
            return

        db = get_database()
        for group in groups:
            name = group['name'] if isinstance(group, sqlite3.Row) else group[0]
            file_path = group['file_path'] if isinstance(group, sqlite3.Row) else group[1]

            conn = self._get_connection()
            try:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, cloud_id, updated_at
                    FROM capsules
                    WHERE name = ? AND file_path = ?
                    ORDER BY updated_at DESC, id DESC
                """, (name, file_path))
                rows = cursor.fetchall()
            finally:
                conn.close()

            if not rows:
                continue

            # ä¼˜å…ˆä¿ç•™æœ‰ cloud_id çš„è®°å½•ï¼Œå…¶æ¬¡æŒ‰ updated_at/ID å–æœ€æ–°
            keep_id = None
            for row in rows:
                row_id = row['id'] if isinstance(row, sqlite3.Row) else row[0]
                row_cloud_id = row['cloud_id'] if isinstance(row, sqlite3.Row) else row[1]
                if row_cloud_id:
                    keep_id = row_id
                    break
            if keep_id is None:
                keep_id = rows[0]['id'] if isinstance(rows[0], sqlite3.Row) else rows[0][0]

            for row in rows:
                row_id = row['id'] if isinstance(row, sqlite3.Row) else row[0]
                if row_id == keep_id:
                    continue
                try:
                    db.delete_capsule(row_id)
                    logger.info(f"ğŸ§¹ å»é‡ï¼šåˆ é™¤é‡å¤èƒ¶å›Š ID {row_id} ({name})")
                except Exception as e:
                    logger.warning(f"âš ï¸ å»é‡å¤±è´¥: {name} (ID {row_id}) - {e}")

    def upload_audio_folders(self, user_id: str, capsule_ids: Optional[List[int]] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸Šä¼ æœ¬åœ° Audio æ–‡ä»¶å¤¹ï¼ˆç”¨äºæ•´ä½“åŒæ­¥ï¼‰
        """
        from supabase_client import get_supabase_client
        from common import PathManager

        supabase = get_supabase_client()
        if not supabase:
            return {
                'success': False,
                'uploaded': 0,
                'errors': ['Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–']
            }

        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            if capsule_ids:
                placeholders = ",".join(["?"] * len(capsule_ids))
                cursor.execute(f"""
                    SELECT id, name, file_path, asset_status
                    FROM capsules
                    WHERE id IN ({placeholders})
                    ORDER BY id
                """, capsule_ids)
            else:
                cursor.execute("""
                    SELECT id, name, file_path, asset_status
                    FROM capsules
                    WHERE asset_status = 'local'
                    ORDER BY id
                """)

            rows = cursor.fetchall()
        finally:
            conn.close()

        if not rows:
            return {'success': True, 'uploaded': 0, 'errors': []}

        pm = PathManager.get_instance()
        uploaded = 0
        errors = []

        for row in rows:
            cap_id, cap_name, cap_file_path, asset_status = row
            capsule_rel_path = cap_file_path or cap_name
            capsule_dir = Path(pm.export_dir) / capsule_rel_path
            audio_dir = capsule_dir / "Audio"

            if not self._has_local_audio_files(capsule_dir):
                continue

            try:
                local_files = [
                    entry for entry in audio_dir.iterdir()
                    if entry.is_file() and not entry.name.startswith('.')
                    and entry.suffix.lower() in ['.wav', '.mp3', '.ogg', '.flac', '.aiff']
                ]
                remote_files = set(supabase.list_audio_files(user_id, capsule_rel_path))
                missing_files = [f for f in local_files if f.name not in remote_files]

                if not missing_files:
                    conn = self._get_connection()
                    try:
                        cursor = conn.cursor()
                        cursor.execute("""
                            UPDATE capsules
                            SET audio_uploaded = 1,
                                updated_at = CURRENT_TIMESTAMP
                            WHERE id = ?
                        """, (cap_id,))
                        conn.commit()
                    finally:
                        conn.close()
                    continue

                result = supabase.upload_audio_files(
                    user_id=user_id,
                    capsule_folder_name=capsule_rel_path,
                    audio_files=missing_files
                )
                if result and result.get('success', False):
                    uploaded += 1
                    conn = self._get_connection()
                    try:
                        cursor = conn.cursor()
                        cursor.execute("""
                            UPDATE capsules
                            SET audio_uploaded = 1,
                                updated_at = CURRENT_TIMESTAMP
                            WHERE id = ?
                        """, (cap_id,))
                        conn.commit()
                    finally:
                        conn.close()
                else:
                    errors.append(f"{cap_name}: Audio ä¸Šä¼ å¤±è´¥")
            except Exception as e:
                errors.append(f"{cap_name}: {e}")

        return {
            'success': len(errors) == 0,
            'uploaded': uploaded,
            'errors': errors
        }

    def sync_tags_only(self, user_id: str) -> Dict[str, Any]:
        """
        åªåŒæ­¥å…³é”®è¯æ•°æ®ï¼ˆcapsule_tagsï¼‰
        
        åŒå‘åŒæ­¥ï¼š
        1. ä¸Šä¼ æœ¬åœ°ä¿®æ”¹è¿‡çš„å…³é”®è¯åˆ°äº‘ç«¯
        2. ä¸‹è½½äº‘ç«¯æ›´æ–°çš„å…³é”®è¯åˆ°æœ¬åœ°
        
        åªåŒæ­¥æœ‰å˜åŒ–çš„æ•°æ®ï¼Œé€šè¿‡ updated_at æ¯”å¯¹
        """
        from supabase_client import get_supabase_client
        from tags_service import get_tags_service
        from capsule_db import get_database

        supabase = get_supabase_client()
        if not supabase:
            return {
                'success': False,
                'uploaded': 0,
                'downloaded': 0,
                'errors': ['Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–']
            }

        uploaded = 0
        downloaded = 0
        errors = []

        try:
            db = get_database()
            tags_service = get_tags_service(db, supabase)
            
            # 1. è·å–æ‰€æœ‰æœ¬åœ°èƒ¶å›ŠåŠå…¶ cloud_id
            conn = self._get_connection()
            try:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, name, cloud_id, updated_at
                    FROM capsules
                    WHERE cloud_id IS NOT NULL AND cloud_id != ''
                """)
                local_capsules = cursor.fetchall()
            finally:
                conn.close()

            logger.info(f"ğŸ·ï¸  å¼€å§‹å…³é”®è¯åŒæ­¥ï¼Œå…± {len(local_capsules)} ä¸ªèƒ¶å›Š")

            for row in local_capsules:
                cap_id, cap_name, cloud_id, local_updated_at = row
                
                try:
                    # 2. è·å–æœ¬åœ°æ ‡ç­¾
                    local_tags = tags_service.get_tags(cap_id)
                    
                    # 2.1 è·å–æœ¬åœ°èšåˆçš„ keywords å­—ç¬¦ä¸²
                    conn_kw = self._get_connection()
                    try:
                        cursor_kw = conn_kw.cursor()
                        cursor_kw.execute("SELECT keywords FROM capsules WHERE id = ?", (cap_id,))
                        row_kw = cursor_kw.fetchone()
                        local_keywords = row_kw[0] if row_kw else None
                    finally:
                        conn_kw.close()
                    
                    # 3. è·å–äº‘ç«¯æ ‡ç­¾
                    cloud_tags = supabase.download_capsule_tags(cloud_id)
                    
                    # 4. æ¯”å¯¹å¹¶å†³å®šåŒæ­¥æ–¹å‘
                    # ç®€å•ç­–ç•¥ï¼šä»¥æœ¬åœ°ä¸ºå‡†ä¸Šä¼ ï¼ˆå› ä¸ºç”¨æˆ·åªåœ¨æœ¬åœ°ä¿®æ”¹ï¼‰
                    if local_tags:
                        # å°†æœ¬åœ°æ ‡ç­¾ä¸Šä¼ åˆ°äº‘ç«¯
                        success = supabase.upload_tags(user_id, cloud_id, local_tags)
                        if success:
                            uploaded += 1
                            logger.info(f"   âœ“ ä¸Šä¼ æ ‡ç­¾: {cap_name} ({len(local_tags)} ä¸ª)")
                            
                            # åŒæ—¶æ›´æ–°äº‘ç«¯ metadata.keywordsï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                            if local_keywords:
                                supabase.update_capsule_keywords(user_id, cap_id, local_keywords)
                                logger.info(f"   âœ“ æ›´æ–°äº‘ç«¯ keywords: {local_keywords[:30]}...")
                        else:
                            errors.append(f"{cap_name}: æ ‡ç­¾ä¸Šä¼ å¤±è´¥")
                    elif cloud_tags:
                        # æœ¬åœ°æ²¡æœ‰æ ‡ç­¾ï¼Œä»äº‘ç«¯ä¸‹è½½
                        conn = self._get_connection()
                        try:
                            cursor = conn.cursor()
                            # å…ˆæ¸…é™¤æœ¬åœ°æ ‡ç­¾
                            cursor.execute("DELETE FROM capsule_tags WHERE capsule_id = ?", (cap_id,))
                            # æ’å…¥äº‘ç«¯æ ‡ç­¾
                            for tag in cloud_tags:
                                cursor.execute("""
                                    INSERT INTO capsule_tags
                                    (capsule_id, lens, word_id, word_cn, word_en, x, y)
                                    VALUES (?, ?, ?, ?, ?, ?, ?)
                                """, (
                                    cap_id,
                                    tag.get('lens') or tag.get('lens_id'),
                                    tag.get('word_id'),
                                    tag.get('word_cn'),
                                    tag.get('word_en'),
                                    tag.get('x'),
                                    tag.get('y')
                                ))
                            conn.commit()
                            downloaded += 1
                            logger.info(f"   âœ“ ä¸‹è½½æ ‡ç­¾: {cap_name} ({len(cloud_tags)} ä¸ª)")
                            
                            # ğŸ”¥ å…³é”®ï¼šä¸‹è½½åç«‹å³èšåˆåˆ° capsules.keywordsï¼ˆç”¨äºæœç´¢ï¼‰
                            self.db.aggregate_and_update_keywords(cap_id)
                            logger.info(f"   âœ“ èšåˆå…³é”®è¯åˆ° capsules.keywords")
                        finally:
                            conn.close()
                            
                except Exception as e:
                    errors.append(f"{cap_name}: {str(e)}")
                    logger.warning(f"   âš ï¸ åŒæ­¥æ ‡ç­¾å¤±è´¥ {cap_name}: {e}")

            # 5. æ¸…é™¤ pending çŠ¶æ€ï¼ˆæ ‡ç­¾å·²åŒæ­¥ï¼‰
            self._clear_tags_pending_status()

            logger.info(f"ğŸ·ï¸  å…³é”®è¯åŒæ­¥å®Œæˆ: ä¸Šä¼  {uploaded}, ä¸‹è½½ {downloaded}")

            return {
                'success': len(errors) == 0,
                'uploaded': uploaded,
                'downloaded': downloaded,
                'errors': errors
            }

        except Exception as e:
            logger.error(f"å…³é”®è¯åŒæ­¥å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'uploaded': uploaded,
                'downloaded': downloaded,
                'errors': [str(e)]
            }

    def _clear_tags_pending_status(self):
        """æ¸…é™¤æ ‡ç­¾ç›¸å…³çš„ pending çŠ¶æ€"""
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE sync_status
                SET sync_state = 'synced',
                    last_sync_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP
                WHERE table_name = 'capsule_tags' AND sync_state = 'pending'
            """)
            conn.commit()
        finally:
            conn.close()

    def _generate_hash(self, data: Dict) -> str:
        """
        ç”Ÿæˆæ•°æ®å“ˆå¸Œå€¼

        Args:
            data: æ•°æ®å­—å…¸

        Returns:
            SHA256 å“ˆå¸Œå€¼
        """
        # å°†å­—å…¸è½¬æ¢ä¸ºæ’åºåçš„ JSON å­—ç¬¦ä¸²
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()

    def mark_for_sync(self, table_name: str, record_id: int, operation: str = 'update') -> bool:
        """
        æ ‡è®°è®°å½•ä¸ºå¾…åŒæ­¥

        Args:
            table_name: è¡¨å
            record_id: è®°å½• ID
            operation: æ“ä½œç±»å‹ ('create', 'update', 'delete')

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("""
                SELECT sync_state FROM sync_status
                WHERE table_name = ? AND record_id = ?
            """, (table_name, record_id))

            existing = cursor.fetchone()

            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                cursor.execute("""
                    UPDATE sync_status
                    SET sync_state = 'pending',
                        updated_at = ?
                    WHERE table_name = ? AND record_id = ?
                """, (datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'), table_name, record_id))
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute("""
                    INSERT INTO sync_status (table_name, record_id, sync_state)
                    VALUES (?, ?, 'pending')
                """, (table_name, record_id))

            # è®°å½•æ—¥å¿—
            cursor.execute("""
                INSERT INTO sync_log (table_name, operation, record_id, direction, status)
                VALUES (?, ?, ?, 'to_cloud', 'pending')
            """, (table_name, operation, record_id))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ æ ‡è®°åŒæ­¥å¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def get_pending_records(self, table_name: str = None) -> List[Dict]:
        """
        è·å–å¾…åŒæ­¥çš„è®°å½•

        Args:
            table_name: è¡¨åï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ‰€æœ‰è¡¨ï¼‰

        Returns:
            å¾…åŒæ­¥è®°å½•åˆ—è¡¨
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            if table_name:
                cursor.execute("""
                    SELECT table_name, record_id, sync_state, local_version, cloud_version
                    FROM sync_status
                    WHERE sync_state = 'pending' AND table_name = ?
                    ORDER BY updated_at ASC
                """, (table_name,))
            else:
                cursor.execute("""
                    SELECT table_name, record_id, sync_state, local_version, cloud_version
                    FROM sync_status
                    WHERE sync_state = 'pending'
                    ORDER BY updated_at ASC
                """)

            return [dict(row) for row in cursor.fetchall()]

        finally:
            conn.close()

    def mark_as_synced(self, table_name: str, record_id: int, cloud_version: int = None) -> bool:
        """
        æ ‡è®°ä¸ºå·²åŒæ­¥

        Args:
            table_name: è¡¨å
            record_id: è®°å½• ID
            cloud_version: äº‘ç«¯ç‰ˆæœ¬å·

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # è·å–å½“å‰è®°å½•
            cursor.execute("""
                SELECT local_version FROM sync_status
                WHERE table_name = ? AND record_id = ?
            """, (table_name, record_id))

            current = cursor.fetchone()
            if not current:
                return False

            # æ›´æ–°ä¸ºå·²åŒæ­¥çŠ¶æ€
            new_local_version = current['local_version'] + 1

            cursor.execute("""
                UPDATE sync_status
                SET sync_state = 'synced',
                    local_version = ?,
                    cloud_version = ?,
                    last_sync_at = ?,
                    updated_at = ?
                WHERE table_name = ? AND record_id = ?
            """, (
                new_local_version,
                cloud_version,
                datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                table_name,
                record_id
            ))

            # è®°å½•æˆåŠŸæ—¥å¿—
            cursor.execute("""
                INSERT INTO sync_log (table_name, operation, record_id, direction, status, local_version, cloud_version)
                VALUES (?, 'sync', ?, 'to_cloud', 'success', ?, ?)
            """, (table_name, record_id, new_local_version, cloud_version))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ æ ‡è®°å·²åŒæ­¥å¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def record_sync_error(self, table_name: str, operation: str, record_id: int, error_message: str) -> bool:
        """
        è®°å½•åŒæ­¥é”™è¯¯

        Args:
            table_name: è¡¨å
            operation: æ“ä½œç±»å‹
            record_id: è®°å½• ID
            error_message: é”™è¯¯ä¿¡æ¯

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO sync_log (table_name, operation, record_id, direction, status, error_message)
                VALUES (?, ?, ?, 'to_cloud', 'failed', ?)
            """, (table_name, operation, record_id, error_message))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ è®°å½•é”™è¯¯å¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def get_sync_status(self) -> Dict[str, Any]:
        """
        è·å–åŒæ­¥çŠ¶æ€æ¦‚è§ˆ

        Returns:
            åŒæ­¥çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«äº‘ç«¯å¾…ä¸‹è½½æ•°é‡
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # ç»Ÿè®¡å„ç§çŠ¶æ€çš„è®°å½•æ•°
            # ğŸ”§ åªç»Ÿè®¡ capsule_tags çš„ pending çŠ¶æ€ï¼Œäº‘å›¾æ ‡åªæ˜¾ç¤ºå…³é”®è¯åŒæ­¥çŠ¶æ€
            cursor.execute("""
                SELECT
                    COUNT(CASE WHEN sync_state = 'synced' THEN 1 END) as synced_count,
                    COUNT(CASE WHEN sync_state = 'pending' AND table_name = 'capsule_tags' THEN 1 END) as pending_count,
                    COUNT(CASE WHEN sync_state = 'conflict' THEN 1 END) as conflict_count,
                    MAX(last_sync_at) as last_sync_at
                FROM sync_status
            """)

            stats = cursor.fetchone()

            # ç»Ÿè®¡æœ¬åœ°èƒ¶å›Šæ€»æ•°
            cursor.execute("SELECT COUNT(*) as local_count FROM capsules")
            local_capsules = cursor.fetchone()['local_count']

            # è·å–äº‘ç«¯èƒ¶å›Šæ•°é‡ï¼ˆé€šè¿‡ cloud_capsules è¡¨æˆ– APIï¼‰
            # è¿™é‡Œæˆ‘ä»¬å…ˆç®€å•å®ç°ï¼šæ£€æŸ¥æœ‰å¤šå°‘ä¸ªäº‘ç«¯èƒ¶å›Šä¸åœ¨æœ¬åœ°
            remote_count = 0
            try:
                from supabase_client import get_supabase_client
                supabase = get_supabase_client()
                if supabase:
                    # è·å–å½“å‰æ¿€æ´»ç”¨æˆ·ï¼ˆä¿®å¤ï¼šä½¿ç”¨ is_active è€Œä¸æ˜¯å›ºå®š id = 1ï¼‰
                    cursor.execute("SELECT supabase_user_id FROM users WHERE is_active = 1")
                    user_row = cursor.fetchone()
                    if user_row and user_row[0]:  # user_row æ˜¯å…ƒç»„ï¼Œä½¿ç”¨ç´¢å¼• [0]
                        user_id = user_row[0]

                        # æŸ¥è¯¢äº‘ç«¯èƒ¶å›Šæ€»æ•°
                        remote_count = supabase.get_capsule_count(user_id)
                        if remote_count is None:
                            remote_count = 0
            except Exception as e:
                # å¦‚æœæŸ¥è¯¢äº‘ç«¯å¤±è´¥ï¼Œremote_count ä¿æŒä¸º 0
                pass

            # è®¡ç®—å¾…ä¸‹è½½æ•°é‡ = äº‘ç«¯æ€»æ•° - æœ¬åœ°å·²åŒæ­¥çš„èƒ¶å›Šæ•°
            # å¦‚æœäº‘ç«¯æœ‰èƒ¶å›Šä½†æœ¬åœ°æ²¡æœ‰ï¼Œåˆ™éœ€è¦ä¸‹è½½
            remote_pending = max(0, remote_count - local_capsules)

            return {
                'synced_count': stats['synced_count'] or 0,
                'pending_count': stats['pending_count'] or 0,
                'conflict_count': stats['conflict_count'] or 0,
                'remote_count': remote_count,  # äº‘ç«¯èƒ¶å›Šæ€»æ•°
                'remote_pending': remote_pending,  # å¾…ä¸‹è½½çš„èƒ¶å›Šæ•°
                'last_sync_at': stats['last_sync_at']
            }

        finally:
            conn.close()

    def detect_conflicts(self, table_name: str, local_data: Dict, cloud_data: Dict) -> Optional[Dict]:
        """
        æ£€æµ‹æ•°æ®å†²çª

        Args:
            table_name: è¡¨å
            local_data: æœ¬åœ°æ•°æ®
            cloud_data: äº‘ç«¯æ•°æ®

        Returns:
            å†²çªä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰å†²çªè¿”å› None
        """
        # ç”Ÿæˆå“ˆå¸Œ
        local_hash = self._generate_hash(local_data)
        cloud_hash = self._generate_hash(cloud_data)

        if local_hash == cloud_hash:
            return None  # æ— å†²çª

        # æ£€æµ‹å†²çªç±»å‹
        if local_data.get('deleted_at') and not cloud_data.get('deleted_at'):
            return {'type': 'delete_conflict', 'local': local_data, 'cloud': cloud_data}
        elif cloud_data.get('deleted_at') and not local_data.get('deleted_at'):
            return {'type': 'delete_conflict', 'local': local_data, 'cloud': cloud_data}
        else:
            return {'type': 'data_conflict', 'local': local_data, 'cloud': cloud_data}

    def record_conflict(self, table_name: str, record_id: int, local_data: Dict, cloud_data: Dict, conflict_type: str) -> bool:
        """
        è®°å½•å†²çª

        Args:
            table_name: è¡¨å
            record_id: è®°å½• ID
            local_data: æœ¬åœ°æ•°æ®
            cloud_data: äº‘ç«¯æ•°æ®
            conflict_type: å†²çªç±»å‹

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO sync_conflicts (table_name, record_id, local_data, cloud_data, conflict_type)
                VALUES (?, ?, ?, ?, ?)
            """, (
                table_name,
                record_id,
                json.dumps(local_data),
                json.dumps(cloud_data),
                conflict_type
            ))

            # æ›´æ–°åŒæ­¥çŠ¶æ€ä¸ºå†²çª
            cursor.execute("""
                UPDATE sync_status
                SET sync_state = 'conflict',
                    updated_at = ?
                WHERE table_name = ? AND record_id = ?
            """, (datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'), table_name, record_id))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ è®°å½•å†²çªå¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def resolve_conflict(self, conflict_id: int, resolution: str) -> bool:
        """
        è§£å†³å†²çª

        Args:
            conflict_id: å†²çªè®°å½• ID
            resolution: è§£å†³æ–¹æ¡ˆ ('local', 'cloud', 'merge')

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # è·å–å†²çªè®°å½•
            cursor.execute("""
                SELECT table_name, record_id, local_data, cloud_data
                FROM sync_conflicts
                WHERE id = ?
            """, (conflict_id,))

            conflict = cursor.fetchone()
            if not conflict:
                return False

            # æ ¹æ®è§£å†³æ–¹æ¡ˆå¤„ç†
            table_name = conflict['table_name']
            record_id = conflict['record_id']

            if resolution == 'local':
                # ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼Œä¸Šä¼ åˆ°äº‘ç«¯
                local_data = json.loads(conflict['local_data'])
                # TODO: å®ç°ä¸Šä¼ é€»è¾‘

            elif resolution == 'cloud':
                # ä½¿ç”¨äº‘ç«¯æ•°æ®ï¼Œä¸‹è½½åˆ°æœ¬åœ°
                cloud_data = json.loads(conflict['cloud_data'])
                # TODO: å®ç°ä¸‹è½½é€»è¾‘

            elif resolution == 'merge':
                # åˆå¹¶æ•°æ®
                # TODO: å®ç°åˆå¹¶é€»è¾‘
                pass

            # æ ‡è®°å†²çªå·²è§£å†³
            cursor.execute("""
                UPDATE sync_conflicts
                SET resolved = 1,
                    resolution = ?,
                    resolved_at = ?
                WHERE id = ?
            """, (resolution, datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'), conflict_id))

            # æ›´æ–°åŒæ­¥çŠ¶æ€
            cursor.execute("""
                UPDATE sync_status
                SET sync_state = 'synced',
                    updated_at = ?
                WHERE table_name = ? AND record_id = ?
            """, (datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'), table_name, record_id))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ è§£å†³å†²çªå¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    # ========== Phase G2: ä»…ä¸‹è½½æ¨¡å¼ï¼ˆå¯åŠ¨åŒæ­¥ä¸“ç”¨ï¼‰ ==========

    def download_only(self, user_id: str, include_previews: bool = True) -> Dict[str, Any]:
        """
        ä»…ä¸‹è½½æ¨¡å¼ï¼šåªä»äº‘ç«¯ä¸‹è½½æ•°æ®ï¼Œä¸ä¸Šä¼ æœ¬åœ°å˜æ›´

        ç”¨é€”ï¼šå¯åŠ¨åŒæ­¥ï¼ˆBootSyncï¼‰ï¼Œé¿å…æ¯æ¬¡å¯åŠ¨éƒ½ä¸Šä¼ æœ¬åœ°æ•°æ®

        Args:
            user_id: Supabase ç”¨æˆ· ID
            include_previews: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½é¢„è§ˆéŸ³é¢‘ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœï¼š{
                'success': bool,
                'downloaded_count': int,
                'preview_downloaded': int,
                'errors': List[str],
                'duration_seconds': float
            }
        """
        import time
        from supabase_client import get_supabase_client

        start_time = time.time()
        errors = []
        downloaded_count = 0
        preview_downloaded = 0

        logger.info("=" * 60)
        logger.info("ğŸ”„ ä»…ä¸‹è½½æ¨¡å¼ï¼ˆå¯åŠ¨åŒæ­¥ï¼‰")
        logger.info("=" * 60)
        logger.info(f"ç”¨æˆ· ID: {user_id}")
        logger.info("âš ï¸  è·³è¿‡æœ¬åœ°æ•°æ®ä¸Šä¼ ")
        logger.info("")

        try:
            # æ­¥éª¤ 1: ä¸‹è½½äº‘ç«¯èƒ¶å›Šå…ƒæ•°æ®
            logger.info("ğŸ“¥ æ­¥éª¤ 1: ä¸‹è½½å…¨çƒèƒ¶å›Šå…ƒæ•°æ®...")
            supabase = get_supabase_client()
            if supabase:
                cloud_capsules = supabase.download_capsules(user_id)

                if cloud_capsules:
                    logger.info(f"   å‘ç° {len(cloud_capsules)} ä¸ªå…¨çƒèƒ¶å›Š")

                    for cloud_capsule in cloud_capsules:
                        try:
                            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ï¼ˆå¤šçº§åŒ¹é…ï¼šcloud_id -> nameï¼‰
                            local_capsule = self._get_local_capsule_by_cloud_id(cloud_capsule['id'])
                            
                            # ğŸ”¥ å¦‚æœ cloud_id åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç”¨ name åŒ¹é…ï¼ˆé˜²æ­¢æœ¬åœ°æ‰«æçš„èƒ¶å›Šæœªå…³è”ï¼‰
                            if not local_capsule:
                                local_capsule = self._get_local_capsule_by_name(cloud_capsule['name'])
                                if local_capsule:
                                    # å…³è” cloud_id
                                    self._set_capsule_cloud_id(local_capsule['id'], cloud_capsule['id'])
                                    logger.info(f"   â„¹ï¸ é€šè¿‡åç§°åŒ¹é…å¹¶å…³è” cloud_id: {cloud_capsule['name']}")

                            if local_capsule:
                                # æ›´æ–°æœ¬åœ°å…ƒæ•°æ®ï¼ˆä¸è¦†ç›–æœ¬åœ°ä¿®æ”¹ï¼‰
                                self._update_local_capsule_metadata(local_capsule['id'], cloud_capsule)
                                owner_id = cloud_capsule.get('user_id', 'unknown')[:8]
                                logger.info(f"   âœ“ æ›´æ–°èƒ¶å›Š {cloud_capsule['name']} (by {owner_id}...)")
                            else:
                                # åˆ›å»ºæ–°èƒ¶å›Šï¼ˆä»…å…ƒæ•°æ®ï¼‰
                                new_capsule_id = self._create_local_capsule_from_cloud(cloud_capsule)
                                owner_id = cloud_capsule.get('user_id', 'unknown')[:8]
                                logger.info(f"   âœ“ æ–°å¢èƒ¶å›Š {cloud_capsule['name']} (by {owner_id}...)")
                                downloaded_count += 1

                        except Exception as e:
                            error_msg = f"åŒæ­¥èƒ¶å›Š {cloud_capsule.get('name', 'Unknown')} å¤±è´¥: {e}"
                            errors.append(error_msg)
                            logger.error(f"   âœ— {error_msg}")
                else:
                    logger.info("   äº‘ç«¯æš‚æ— èƒ¶å›Šæ•°æ®")
            else:
                logger.warning("   âš ï¸  Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡äº‘ç«¯ä¸‹è½½")

            logger.info("")

            # æ­¥éª¤ 2: ä¸‹è½½è½»é‡èµ„äº§æ–‡ä»¶ï¼ˆOGG é¢„è§ˆ + RPP é¡¹ç›®æ–‡ä»¶ï¼‰
            logger.info("ğŸ“¥ æ­¥éª¤ 2: ä¸‹è½½è½»é‡èµ„äº§æ–‡ä»¶ï¼ˆOGG + RPPï¼‰...")

            from capsule_db import get_database
            db = get_database()
            db.connect()
            cursor = db.conn.cursor()

            cursor.execute("""
                SELECT id, name, uuid, preview_audio, cloud_status, asset_status,
                       owner_supabase_user_id, cloud_id, file_path
                FROM capsules
                WHERE cloud_id IS NOT NULL
                ORDER BY id
            """)
            local_capsules = cursor.fetchall()

            db.close()

            if local_capsules:
                logger.info(f"   æ£€æŸ¥ {len(local_capsules)} ä¸ªèƒ¶å›Šçš„è½»é‡èµ„äº§...")

                for idx, cap in enumerate(local_capsules, 1):
                    cap_id, cap_name, cap_uuid, preview_audio, cloud_status, asset_status, owner_id, cloud_id, cap_file_path = cap

                    # å‡†å¤‡æœ¬åœ°è·¯å¾„ - ä» PathManager è·å–å¯¼å‡ºç›®å½•
                    from common import PathManager
                    pm = PathManager.get_instance()
                    export_dir = pm.export_dir
                    logger.info(f"   ä½¿ç”¨å¯¼å‡ºç›®å½•: {export_dir}")

                    capsule_rel_path = cap_file_path or cap_name
                    capsule_dir = Path(export_dir) / capsule_rel_path

                    # âœ… çŠ¶æ€è‡ªæ„ˆï¼šå¦‚æœæœ¬åœ°æœ‰ Audio æ–‡ä»¶å¤¹ï¼Œæ›´æ–° asset_status
                    if self._has_local_audio_files(capsule_dir):
                        if self._update_asset_status_if_needed(cap_id, asset_status, 'local'):
                            logger.info(f"   âœ¨ æ£€æµ‹åˆ°æœ¬åœ°éŸ³é¢‘ï¼Œä¿®æ­£èµ„äº§çŠ¶æ€: {cap_name} -> local")
                        asset_status = 'local'
                    needs_download = []

                    try:
                        # æ£€æŸ¥ metadata.json æ–‡ä»¶
                        metadata_path = capsule_dir / "metadata.json"
                        if not metadata_path.exists():
                            needs_download.append(('metadata', 'metadata.json'))
                        
                        # æ£€æŸ¥ OGG é¢„è§ˆæ–‡ä»¶
                        if preview_audio:
                            ogg_path = capsule_dir / preview_audio
                            if not ogg_path.exists():
                                needs_download.append(('preview', preview_audio))

                        # æ£€æŸ¥ RPP é¡¹ç›®æ–‡ä»¶ï¼ˆä½¿ç”¨èƒ¶å›Šåç§°ï¼‰
                        rpp_filename = f"{cap_name}.rpp"
                        rpp_path = capsule_dir / rpp_filename
                        if not rpp_path.exists():
                            needs_download.append(('rpp', rpp_filename))

                        # ä¸‹è½½ç¼ºå¤±çš„æ–‡ä»¶
                        if needs_download and owner_id:
                            if not supabase:
                                logger.warning(f"   âš ï¸  Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ–‡ä»¶ä¸‹è½½")
                                break

                            for file_type, filename in needs_download:
                                try:
                                    print(f"   [{idx}/{len(local_capsules)}] ä¸‹è½½ {filename}", end='\r')

                                    # æ„å»ºæœ¬åœ°è·¯å¾„
                                    local_path = capsule_dir / filename

                                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                                    capsule_dir.mkdir(parents=True, exist_ok=True)

                                    # è°ƒç”¨ä¸‹è½½
                                    # æ³¨æ„ï¼šäº‘ç«¯æ–‡ä»¶å¤¹ä½¿ç”¨èƒ¶å›Šåç§°ï¼Œè€Œä¸æ˜¯ uuid
                                    success = supabase.download_file(
                                        user_id=owner_id,
                                        capsule_folder_name=cap_name,
                                        file_type=file_type,
                                        local_path=str(local_path)
                                    )

                                    if success:
                                        if file_type == 'preview':
                                            preview_downloaded += 1
                                        logger.info(f"   âœ“ [{idx}/{len(local_capsules)}] {filename}")
                                    else:
                                        logger.warning(f"   âœ— [{idx}/{len(local_capsules)}] {filename} ä¸‹è½½å¤±è´¥")

                                except Exception as e:
                                    error_msg = f"ä¸‹è½½ {cap_name}/{filename} å¤±è´¥: {e}"
                                    errors.append(error_msg)
                                    logger.error(f"   âœ— {error_msg}")

                        # ğŸ·ï¸ å¤„ç† Tagsï¼šä¼˜å…ˆä½¿ç”¨äº‘ç«¯æ•°æ®åº“ï¼Œæ–‡ä»¶ä½œä¸ºå¤‡ä»½
                        try:
                            from tags_service import get_tags_service
                            tags_service = get_tags_service()
                            
                            # ä½¿ç”¨å·²è§£åŒ…çš„ cloud_id å˜é‡
                            if cloud_id and supabase:
                                # å°è¯•ä»äº‘ç«¯æ•°æ®åº“æ‹‰å– Tags
                                logger.info(f"   ğŸ·ï¸  å°è¯•ä»äº‘ç«¯æ•°æ®åº“æ‹‰å– Tags...")
                                tags_synced = tags_service.sync_tags_from_cloud(cap_id, cloud_id)
                                
                                if tags_synced:
                                    logger.info(f"   âœ“ Tags å·²ä»äº‘ç«¯åŒæ­¥")
                                else:
                                    # å¦‚æœäº‘ç«¯æ²¡æœ‰ Tagsï¼Œå°è¯•ä» metadata.json å¯¼å…¥
                                    metadata_path = capsule_dir / "metadata.json"
                                    if metadata_path.exists():
                                        logger.info(f"   âš ï¸  äº‘ç«¯æ—  Tagsï¼Œå°è¯•ä» metadata.json å¯¼å…¥...")
                                        tags_service.merge_tags_from_metadata(cap_id, metadata_path)
                            else:
                                # ç¦»çº¿æ¨¡å¼ï¼šä» metadata.json å¯¼å…¥
                                metadata_path = capsule_dir / "metadata.json"
                                if metadata_path.exists():
                                    logger.info(f"   â„¹ï¸  ç¦»çº¿æ¨¡å¼ï¼Œä» metadata.json å¯¼å…¥ Tags...")
                                    tags_service.merge_tags_from_metadata(cap_id, metadata_path)
                        except Exception as e:
                            logger.warning(f"   âš ï¸  Tags å¤„ç†å¤±è´¥: {e}")

                        # ğŸ“Š å¤„ç†æŠ€æœ¯å…ƒæ•°æ®ï¼šä» metadata.json å†™å…¥ capsule_metadata è¡¨
                        try:
                            metadata_path = capsule_dir / "metadata.json"
                            if metadata_path.exists():
                                self._save_metadata_to_db(cap_id, metadata_path)
                        except Exception as e:
                            logger.warning(f"   âš ï¸  å…ƒæ•°æ®å†™å…¥å¤±è´¥: {e}")

                    except Exception as e:
                        error_msg = f"æ£€æŸ¥ {cap_name} èµ„äº§å¤±è´¥: {e}"
                        errors.append(error_msg)
                        logger.error(f"   âœ— {error_msg}")

                logger.info("")
                logger.info(f"   âœ“ é¢„è§ˆéŸ³é¢‘ä¸‹è½½: {preview_downloaded} ä¸ª")
                logger.info("")

            else:
                logger.info("   âœ“ æ— éœ€ä¸‹è½½èµ„äº§ï¼ˆæœ¬åœ°æš‚æ— èƒ¶å›Šï¼‰")
                logger.info("")

        except Exception as e:
            error_msg = f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}"
            errors.append(error_msg)
            logger.error(f"âŒ {error_msg}")

        # è®¡ç®—è€—æ—¶
        duration = time.time() - start_time

        # æ‰“å°æ€»ç»“
        logger.info("=" * 60)
        logger.info("ğŸ“Š ä»…ä¸‹è½½å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"ä¸‹è½½èƒ¶å›Šæ•°: {downloaded_count}")
        logger.info(f"é¢„è§ˆéŸ³é¢‘ä¸‹è½½: {preview_downloaded}")
        logger.info(f"é”™è¯¯æ•°é‡: {len(errors)}")
        logger.info(f"è€—æ—¶: {duration:.2f} ç§’")

        if errors:
            logger.info("")
            logger.error("é”™è¯¯è¯¦æƒ…:")
            for error in errors:
                logger.error(f"  â€¢ {error}")

        logger.info("=" * 60)
        logger.info("")

        return {
            'success': len(errors) == 0,
            'downloaded_count': downloaded_count,
            'preview_downloaded': preview_downloaded,
            'errors': errors,
            'duration_seconds': duration
        }

    # ========== Phase B.4: è½»é‡çº§åŒæ­¥ï¼ˆå…ƒæ•°æ® + é¢„è§ˆéŸ³é¢‘ï¼‰ ==========

    def sync_metadata_lightweight(self, user_id: str, include_previews: bool = True, capsule_ids: list = None) -> Dict[str, Any]:
        """
        è½»é‡çº§åŒæ­¥ï¼šä»…åŒæ­¥å…ƒæ•°æ® + é¢„è§ˆéŸ³é¢‘ï¼ˆå¯é€‰ï¼‰

        Args:
            user_id: Supabase ç”¨æˆ· ID
            include_previews: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½é¢„è§ˆéŸ³é¢‘ï¼ˆé»˜è®¤ Trueï¼‰
            capsule_ids: æŒ‡å®šè¦åŒæ­¥çš„èƒ¶å›Š ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸º None åˆ™åŒæ­¥æ‰€æœ‰ï¼‰

        Returns:
            åŒæ­¥ç»“æœï¼š{
                'success': bool,
                'synced_count': int,
                'preview_downloaded': int,
                'errors': List[str],
                'duration_seconds': float
            }
        """
        import time
        from supabase_client import get_supabase_client

        start_time = time.time()
        errors = []
        synced_count = 0
        preview_downloaded = 0

        print("=" * 60)
        print("ğŸ”„ è½»é‡çº§åŒæ­¥å¼€å§‹")
        print("=" * 60)
        print(f"ç”¨æˆ· ID: {user_id}")
        print(f"åŒ…å«é¢„è§ˆéŸ³é¢‘: {include_previews}")
        print()

        try:
            # 0. å¯åŠ¨åŒæ­¥å‰å…ˆå»é‡æœ¬åœ°èƒ¶å›Šï¼ˆé¿å…åŒåé‡å¤ï¼‰
            self._dedupe_local_capsules()

            # 1. ä¸Šä¼ æœ¬åœ°å˜æ›´ï¼ˆå…ƒæ•°æ® + æ–‡ä»¶ï¼‰
            print("ğŸ“¤ æ­¥éª¤ 1: ä¸Šä¼ æœ¬åœ°å…ƒæ•°æ®å˜æ›´...")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœæŒ‡å®šäº† capsule_idsï¼Œå¼ºåˆ¶ä¸Šä¼ è¿™äº›èƒ¶å›Šï¼ˆå¿½ç•¥åŒæ­¥çŠ¶æ€ï¼‰
            if capsule_ids:
                print(f"   ğŸ¯ å¼ºåˆ¶ä¸Šä¼ æŒ‡å®šçš„ {len(capsule_ids)} ä¸ªèƒ¶å›Šï¼ˆå¿½ç•¥åŒæ­¥çŠ¶æ€ï¼‰")
                # ç›´æ¥æ„é€ å¾…ä¸Šä¼ åˆ—è¡¨ï¼Œä¸æ£€æŸ¥ sync_status
                local_pending = [{'record_id': cid} for cid in capsule_ids]
            else:
                # æ­£å¸¸æµç¨‹ï¼šåªä¸Šä¼ æœªåŒæ­¥çš„èƒ¶å›Š
                local_pending = self.get_pending_records('capsules')

            if local_pending:
                print(f"   å‘ç° {len(local_pending)} ä¸ªå¾…ä¸Šä¼ çš„èƒ¶å›Š")

                supabase = get_supabase_client()
                if not supabase:
                    print("   âš ï¸  Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                    return {
                        'success': False,
                        'synced_count': 0,
                        'preview_downloaded': 0,
                        'errors': ['Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–'],
                        'duration_seconds': time.time() - start_time
                    }

                for record in local_pending:
                    try:
                        record_id = record['record_id']
                        print(f"\n   ğŸ” [DEBUG] å¼€å§‹å¤„ç†èƒ¶å›Š ID: {record_id}")
                        _set_upload_progress(record_id, {
                            'status': 'uploading',
                            'stage': 'å‡†å¤‡ä¸Šä¼ ',
                            'percent': 5,
                            'message': 'å‡†å¤‡ä¸Šä¼ èƒ¶å›Šæ•°æ®...'
                        })
                        
                        # è·å–æœ¬åœ°èƒ¶å›Šæ•°æ®ï¼ˆä»…å…ƒæ•°æ®ï¼Œä¸å« WAVï¼‰
                        capsule_data = self._get_capsule_metadata_only(record_id)
                        capsule_name = capsule_data.get('name', 'Unknown')
                        capsule_dir = capsule_data.get('file_path', '')
                        
                        print(f"   ğŸ” [DEBUG] èƒ¶å›Šåç§°: {capsule_name}")
                        print(f"   ğŸ” [DEBUG] èƒ¶å›Šç›®å½•: {capsule_dir}")

                        # ä¸Šä¼ å…ƒæ•°æ®åˆ° Supabase Databaseï¼ˆä»… keywords æ›´æ–°ï¼‰
                        print(f"   ğŸ” [DEBUG] å‡†å¤‡ä¸Šä¼ å…ƒæ•°æ®åˆ° Database...")
                        # ğŸ”¥ ä¼ å…¥èƒ¶å›Šåç§°ï¼Œé˜²æ­¢åˆ‡æ¢æ–‡ä»¶å¤¹å local_id å˜åŒ–å¯¼è‡´é‡å¤ä¸Šä¼ 
                        existing_cloud = supabase.get_cloud_capsule_by_local_id(user_id, capsule_data.get('id'), capsule_name)
                        result = None
                        if existing_cloud:
                            cloud_id = existing_cloud.get('id')
                            remote_meta = existing_cloud.get('metadata') or {}
                            remote_keywords = remote_meta.get('keywords') if isinstance(remote_meta, dict) else None
                            local_keywords = capsule_data.get('keywords')
                            if local_keywords != remote_keywords:
                                result = supabase.update_capsule_keywords(user_id, capsule_data.get('id'), local_keywords)
                            else:
                                result = existing_cloud
                        else:
                            result = supabase.upload_capsule(user_id, capsule_data)
                        print(f"   ğŸ” [DEBUG] å…ƒæ•°æ®ä¸Šä¼ ç»“æœ: {result is not None}")

                        if result:
                            cloud_id = result.get('id') if result else None
                            print(f"   âœ“ ä¸Šä¼ èƒ¶å›Šå…ƒæ•°æ®: {capsule_name} (cloud_id={cloud_id})")
                            _set_upload_progress(record_id, {
                                'status': 'uploading',
                                'stage': 'ä¸Šä¼ å…ƒæ•°æ®',
                                'percent': 10,
                                'message': 'å…ƒæ•°æ®ä¸Šä¼ å®Œæˆ'
                            })
                            
                            # ğŸ”§ ç«‹å³æ›´æ–° cloud_idï¼ˆé˜²æ­¢æ–‡ä»¶ä¸Šä¼ å¤±è´¥å¯¼è‡´æ•°æ®ä¸ä¸€è‡´ï¼‰
                            conn = self._get_connection()
                            try:
                                cursor = conn.cursor()
                                cursor.execute("""
                                    UPDATE capsules
                                    SET cloud_id = ?,
                                        cloud_version = ?
                                    WHERE id = ?
                                """, (cloud_id, result.get('version', 1), record_id))
                                conn.commit()
                                print(f"   âœ“ å·²è®¾ç½® cloud_id")
                            finally:
                                conn.close()
                            
                            # ğŸ“ ä¸Šä¼ æ–‡ä»¶åˆ° Supabase Storageï¼ˆåŸå­åŒ–æ“ä½œï¼‰
                            from common import PathManager
                            pm = PathManager.get_instance()
                            full_capsule_dir = pm.export_dir / capsule_dir
                            
                            print(f"\n   ğŸ” [DEBUG] ========== æ–‡ä»¶ä¸Šä¼ æ£€æŸ¥ ==========")
                            print(f"   ğŸ” [DEBUG] å¯¼å‡ºç›®å½•: {pm.export_dir}")
                            print(f"   ğŸ” [DEBUG] èƒ¶å›Šç›®å½•: {capsule_dir}")
                            print(f"   ğŸ” [DEBUG] å®Œæ•´è·¯å¾„: {full_capsule_dir}")
                            print(f"   ğŸ” [DEBUG] ç›®å½•æ˜¯å¦å­˜åœ¨? {full_capsule_dir.exists()}")
                            
                            # ğŸ”’ åŸå­åŒ–æ“ä½œï¼šæ‰€æœ‰æ–‡ä»¶å¿…é¡»å…¨éƒ¨ä¸Šä¼ æˆåŠŸ
                            all_files_uploaded = True
                            upload_errors = []
                            
                            if full_capsule_dir.exists():
                                print(f"   ğŸ” [DEBUG] âœ“ ç›®å½•å­˜åœ¨ï¼Œå¼€å§‹æ£€æŸ¥æ–‡ä»¶...")
                                _set_upload_progress(record_id, {
                                    'status': 'uploading',
                                    'stage': 'æ£€æŸ¥æ–‡ä»¶',
                                    'percent': 15,
                                    'message': 'æ£€æŸ¥æœ¬åœ°æ–‡ä»¶...'
                                })
                                
                                # ğŸµ ä¸Šä¼ é¢„è§ˆéŸ³é¢‘
                                preview_audio = capsule_data.get('preview_audio')
                                print(f"   ğŸ” [DEBUG] é¢„è§ˆéŸ³é¢‘æ–‡ä»¶å: {preview_audio}")
                                if preview_audio:
                                    preview_path = full_capsule_dir / preview_audio
                                    print(f"   ğŸ” [DEBUG] é¢„è§ˆéŸ³é¢‘è·¯å¾„: {preview_path}")
                                    print(f"   ğŸ” [DEBUG] é¢„è§ˆéŸ³é¢‘å­˜åœ¨? {preview_path.exists()}")
                                    if preview_path.exists():
                                        preview_exists = supabase.storage_file_exists(user_id, capsule_dir, preview_audio)
                                        if preview_exists:
                                            print(f"   âœ“ é¢„è§ˆéŸ³é¢‘å·²å­˜åœ¨äºäº‘ç«¯ï¼Œè·³è¿‡ä¸Šä¼ ")
                                            _set_upload_progress(record_id, {
                                                'status': 'uploading',
                                                'stage': 'ä¸Šä¼ é¢„è§ˆéŸ³é¢‘',
                                                'percent': 20,
                                                'message': 'é¢„è§ˆéŸ³é¢‘å·²å­˜åœ¨'
                                            })
                                        else:
                                            print(f"   â†’ ä¸Šä¼ é¢„è§ˆéŸ³é¢‘: {preview_audio}")
                                            try:
                                                preview_result = supabase.upload_file(
                                                    user_id=user_id,
                                                    capsule_folder_name=capsule_dir,
                                                    file_type='preview',
                                                    file_path=str(preview_path)
                                                )
                                                if preview_result:
                                                    print(f"   âœ“ é¢„è§ˆéŸ³é¢‘ä¸Šä¼ æˆåŠŸ")
                                                    _set_upload_progress(record_id, {
                                                        'status': 'uploading',
                                                        'stage': 'ä¸Šä¼ é¢„è§ˆéŸ³é¢‘',
                                                        'percent': 20,
                                                        'message': 'é¢„è§ˆéŸ³é¢‘ä¸Šä¼ å®Œæˆ'
                                                    })
                                                else:
                                                    print(f"   âœ— é¢„è§ˆéŸ³é¢‘ä¸Šä¼ å¤±è´¥")
                                                    all_files_uploaded = False
                                                    upload_errors.append("é¢„è§ˆéŸ³é¢‘ä¸Šä¼ å¤±è´¥")
                                            except Exception as e:
                                                print(f"   âœ— é¢„è§ˆéŸ³é¢‘ä¸Šä¼ å¼‚å¸¸: {e}")
                                                all_files_uploaded = False
                                                upload_errors.append(f"é¢„è§ˆéŸ³é¢‘ä¸Šä¼ å¼‚å¸¸: {e}")
                                    else:
                                        print(f"   âœ— é¢„è§ˆéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                                        all_files_uploaded = False
                                        upload_errors.append("é¢„è§ˆéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                                
                                # ğŸ“„ ä¸Šä¼  RPP é¡¹ç›®æ–‡ä»¶
                                rpp_file = capsule_data.get('rpp_file')
                                print(f"   ğŸ” [DEBUG] RPP æ–‡ä»¶å: {rpp_file}")
                                if rpp_file:
                                    rpp_path = full_capsule_dir / rpp_file
                                    print(f"   ğŸ” [DEBUG] RPP è·¯å¾„: {rpp_path}")
                                    print(f"   ğŸ” [DEBUG] RPP å­˜åœ¨? {rpp_path.exists()}")
                                    if rpp_path.exists():
                                        rpp_exists = supabase.storage_file_exists(user_id, capsule_dir, rpp_file)
                                        if rpp_exists:
                                            print(f"   âœ“ RPP å·²å­˜åœ¨äºäº‘ç«¯ï¼Œè·³è¿‡ä¸Šä¼ ")
                                            _set_upload_progress(record_id, {
                                                'status': 'uploading',
                                                'stage': 'ä¸Šä¼  RPP',
                                                'percent': 30,
                                                'message': 'RPP å·²å­˜åœ¨'
                                            })
                                        else:
                                            print(f"   â†’ ä¸Šä¼  RPP æ–‡ä»¶: {rpp_file}")
                                            try:
                                                rpp_result = supabase.upload_file(
                                                    user_id=user_id,
                                                    capsule_folder_name=capsule_dir,
                                                    file_type='rpp',
                                                    file_path=str(rpp_path)
                                                )
                                                if rpp_result:
                                                    print(f"   âœ“ RPP æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                                                    _set_upload_progress(record_id, {
                                                        'status': 'uploading',
                                                        'stage': 'ä¸Šä¼  RPP',
                                                        'percent': 30,
                                                        'message': 'RPP ä¸Šä¼ å®Œæˆ'
                                                    })
                                                else:
                                                    print(f"   âœ— RPP æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                                                    all_files_uploaded = False
                                                    upload_errors.append("RPP æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                                            except Exception as e:
                                                print(f"   âœ— RPP æ–‡ä»¶ä¸Šä¼ å¼‚å¸¸: {e}")
                                                all_files_uploaded = False
                                                upload_errors.append(f"RPP æ–‡ä»¶ä¸Šä¼ å¼‚å¸¸: {e}")
                                    else:
                                        print(f"   âœ— RPP æ–‡ä»¶ä¸å­˜åœ¨")
                                        all_files_uploaded = False
                                        upload_errors.append("RPP æ–‡ä»¶ä¸å­˜åœ¨")
                                
                                # ğŸ“‹ ä¸Šä¼  metadata.json æ–‡ä»¶
                                metadata_file = full_capsule_dir / "metadata.json"
                                print(f"   ğŸ” [DEBUG] metadata.json è·¯å¾„: {metadata_file}")
                                print(f"   ğŸ” [DEBUG] metadata.json å­˜åœ¨? {metadata_file.exists()}")
                                if metadata_file.exists():
                                    metadata_exists = supabase.storage_file_exists(user_id, capsule_dir, "metadata.json")
                                    if metadata_exists:
                                        print(f"   âœ“ metadata.json å·²å­˜åœ¨äºäº‘ç«¯ï¼Œè·³è¿‡ä¸Šä¼ ")
                                        _set_upload_progress(record_id, {
                                            'status': 'uploading',
                                            'stage': 'ä¸Šä¼  metadata.json',
                                            'percent': 40,
                                            'message': 'metadata.json å·²å­˜åœ¨'
                                        })
                                    else:
                                        print(f"   â†’ ä¸Šä¼  metadata.json...")
                                        try:
                                            metadata_result = supabase.upload_file(
                                                user_id=user_id,
                                                capsule_folder_name=capsule_dir,
                                                file_type='metadata',
                                                file_path=str(metadata_file)
                                            )
                                            if metadata_result:
                                                print(f"   âœ“ metadata.json ä¸Šä¼ æˆåŠŸ")
                                                _set_upload_progress(record_id, {
                                                    'status': 'uploading',
                                                    'stage': 'ä¸Šä¼  metadata.json',
                                                    'percent': 40,
                                                    'message': 'metadata.json ä¸Šä¼ å®Œæˆ'
                                                })
                                            else:
                                                print(f"   âœ— metadata.json ä¸Šä¼ å¤±è´¥")
                                                all_files_uploaded = False
                                                upload_errors.append("metadata.json ä¸Šä¼ å¤±è´¥")
                                        except Exception as e:
                                            print(f"   âœ— metadata.json ä¸Šä¼ å¼‚å¸¸: {e}")
                                            all_files_uploaded = False
                                            upload_errors.append(f"metadata.json ä¸Šä¼ å¼‚å¸¸: {e}")
                                else:
                                    print(f"   âš  metadata.json ä¸å­˜åœ¨ï¼ˆå¯é€‰æ–‡ä»¶ï¼Œä¸å½±å“åŒæ­¥ï¼‰")

                                # ğŸ§ ä¸Šä¼  Audio æ–‡ä»¶å¤¹ï¼ˆå®Œæ•´æ•°æ®ï¼‰
                                audio_folder = full_capsule_dir / "Audio"
                                print(f"   ğŸ” [DEBUG] Audio è·¯å¾„: {audio_folder}")
                                print(f"   ğŸ” [DEBUG] Audio å­˜åœ¨? {audio_folder.exists()}")
                                if audio_folder.exists() and audio_folder.is_dir():
                                    audio_files = [
                                        entry for entry in audio_folder.iterdir()
                                        if entry.is_file() and not entry.name.startswith('.')
                                        and entry.suffix.lower() in ['.wav', '.mp3', '.ogg', '.flac', '.aiff']
                                    ]
                                    has_audio_files = len(audio_files) > 0
                                    if has_audio_files:
                                        remote_files = set(supabase.list_audio_files(user_id, capsule_dir))
                                        missing_files = [f for f in audio_files if f.name not in remote_files]
                                        if not missing_files:
                                            print(f"   âœ“ Audio æ–‡ä»¶å¤¹å·²å®Œæ•´å­˜åœ¨äºäº‘ç«¯ï¼Œè·³è¿‡ä¸Šä¼ ")
                                            conn = self._get_connection()
                                            try:
                                                cursor = conn.cursor()
                                                cursor.execute("""
                                                    UPDATE capsules
                                                    SET audio_uploaded = 1,
                                                        updated_at = CURRENT_TIMESTAMP
                                                    WHERE id = ?
                                                """, (record_id,))
                                                conn.commit()
                                            finally:
                                                conn.close()
                                            _set_upload_progress(record_id, {
                                                'status': 'uploading',
                                                'stage': 'ä¸Šä¼  Audio',
                                                'percent': 100,
                                                'message': 'Audio å·²å­˜åœ¨'
                                            })
                                        else:
                                            print(f"   â†’ ä¸Šä¼  Audio æ–‡ä»¶å¤¹ï¼ˆç¼ºå¤± {len(missing_files)} ä¸ªæ–‡ä»¶ï¼‰...")
                                            try:
                                                total_files = len(missing_files)

                                                def _audio_progress(uploaded, total, filename):
                                                    if total <= 0:
                                                        percent = 95
                                                    else:
                                                        percent = 40 + int((uploaded / total) * 60)
                                                    _set_upload_progress(record_id, {
                                                        'status': 'uploading',
                                                        'stage': 'ä¸Šä¼  Audio',
                                                        'percent': min(99, percent),
                                                        'current_file': filename,
                                                        'uploaded_files': uploaded,
                                                        'total_files': total,
                                                        'message': 'ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶...'
                                                    })

                                                audio_result = supabase.upload_audio_files(
                                                    user_id=user_id,
                                                    capsule_folder_name=capsule_dir,
                                                    audio_files=missing_files,
                                                    progress_callback=_audio_progress
                                                )
                                                if audio_result and audio_result.get('success', False):
                                                    print(f"   âœ“ Audio æ–‡ä»¶å¤¹ä¸Šä¼ æˆåŠŸ")
                                                    conn = self._get_connection()
                                                    try:
                                                        cursor = conn.cursor()
                                                        cursor.execute("""
                                                            UPDATE capsules
                                                            SET audio_uploaded = 1,
                                                                updated_at = CURRENT_TIMESTAMP
                                                            WHERE id = ?
                                                        """, (record_id,))
                                                        conn.commit()
                                                    finally:
                                                        conn.close()
                                                    _set_upload_progress(record_id, {
                                                        'status': 'uploading',
                                                        'stage': 'ä¸Šä¼  Audio',
                                                        'percent': 100,
                                                        'message': 'Audio ä¸Šä¼ å®Œæˆ'
                                                    })
                                                else:
                                                    print(f"   âœ— Audio æ–‡ä»¶å¤¹ä¸Šä¼ å¤±è´¥")
                                                    all_files_uploaded = False
                                                    upload_errors.append("Audio æ–‡ä»¶å¤¹ä¸Šä¼ å¤±è´¥")
                                            except Exception as e:
                                                print(f"   âœ— Audio æ–‡ä»¶å¤¹ä¸Šä¼ å¼‚å¸¸: {e}")
                                                all_files_uploaded = False
                                                upload_errors.append(f"Audio æ–‡ä»¶å¤¹ä¸Šä¼ å¼‚å¸¸: {e}")
                                    else:
                                        print(f"   âœ— Audio æ–‡ä»¶å¤¹ä¸ºç©º")
                                        all_files_uploaded = False
                                        upload_errors.append("Audio æ–‡ä»¶å¤¹ä¸ºç©º")
                                else:
                                    print(f"   âœ— Audio æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
                                    all_files_uploaded = False
                                    upload_errors.append("Audio æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
                            else:
                                print(f"   âœ— èƒ¶å›Šç›®å½•ä¸å­˜åœ¨: {full_capsule_dir}")
                                all_files_uploaded = False
                                upload_errors.append("èƒ¶å›Šç›®å½•ä¸å­˜åœ¨")
                            
                            # ğŸ”’ å…³é”®å†³æ–­ç‚¹ï¼šåªæœ‰æ‰€æœ‰æ–‡ä»¶éƒ½ä¸Šä¼ æˆåŠŸï¼Œæ‰æ ‡è®°ä¸º synced
                            if all_files_uploaded:
                                # ğŸ·ï¸ è‡ªåŠ¨ä¸Šä¼ å…³é”®è¯åˆ° cloud_capsule_tags è¡¨
                                # ç¡®ä¿å…¶ä»–ç”¨æˆ·åŒæ­¥åèƒ½çœ‹åˆ°å…³é”®è¯
                                try:
                                    conn_tags = self._get_connection()
                                    cursor_tags = conn_tags.cursor()
                                    cursor_tags.execute("""
                                        SELECT lens, word_id, word_cn, word_en, x, y
                                        FROM capsule_tags
                                        WHERE capsule_id = ?
                                    """, (record_id,))
                                    local_tags = []
                                    for row in cursor_tags.fetchall():
                                        local_tags.append({
                                            'lens': row[0],
                                            'word_id': row[1],
                                            'word_cn': row[2],
                                            'word_en': row[3],
                                            'x': row[4],
                                            'y': row[5],
                                        })
                                    conn_tags.close()
                                    
                                    if local_tags and cloud_id:
                                        print(f"   ğŸ·ï¸  ä¸Šä¼  {len(local_tags)} ä¸ªå…³é”®è¯åˆ° cloud_capsule_tags...")
                                        tags_uploaded = supabase.upload_tags(user_id, cloud_id, local_tags)
                                        if tags_uploaded:
                                            print(f"   âœ“ å…³é”®è¯ä¸Šä¼ æˆåŠŸ")
                                        else:
                                            print(f"   âš ï¸ å…³é”®è¯ä¸Šä¼ å¤±è´¥ï¼ˆä¸å½±å“èƒ¶å›ŠåŒæ­¥çŠ¶æ€ï¼‰")
                                    elif not local_tags:
                                        print(f"   â„¹ï¸  è¯¥èƒ¶å›Šæš‚æ— å…³é”®è¯")
                                except Exception as tags_err:
                                    print(f"   âš ï¸ ä¸Šä¼ å…³é”®è¯å¼‚å¸¸: {tags_err}ï¼ˆä¸å½±å“èƒ¶å›ŠåŒæ­¥çŠ¶æ€ï¼‰")
                                
                                # æ›´æ–° sync_status è¡¨
                                self.mark_as_synced('capsules', record_id)
                                
                                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåŒæ—¶æ›´æ–° capsules è¡¨çš„ cloud_status å­—æ®µ
                                # å‰ç«¯é€šè¿‡ capsule.cloud_status åˆ¤æ–­çŠ¶æ€ï¼Œå¿…é¡»æ›´æ–°æ­¤å­—æ®µ
                                conn = self._get_connection()
                                try:
                                    cursor = conn.cursor()
                                    cursor.execute("""
                                        UPDATE capsules
                                        SET cloud_status = 'synced',
                                            last_synced_at = CURRENT_TIMESTAMP
                                        WHERE id = ?
                                    """, (record_id,))
                                    conn.commit()
                                    print(f"   âœ“ å·²æ›´æ–° capsules.cloud_status = 'synced'")
                                finally:
                                    conn.close()
                                
                                synced_count += 1
                                print(f"   âœ… èƒ¶å›Š {record_id} å®Œå…¨åŒæ­¥æˆåŠŸ")
                                _set_upload_progress(record_id, {
                                    'status': 'completed',
                                    'stage': 'å®Œæˆ',
                                    'percent': 100,
                                    'message': 'ä¸Šä¼ å®Œæˆ'
                                })
                            else:
                                error_msg = f"èƒ¶å›Š {record_id} æ–‡ä»¶ä¸Šä¼ ä¸å®Œæ•´: {', '.join(upload_errors)}"
                                errors.append(error_msg)
                                print(f"   âš ï¸ {error_msg}")
                                print(f"   â„¹ï¸  çŠ¶æ€ä¿æŒä¸º 'local'ï¼Œä¸‹æ¬¡åŒæ­¥æ—¶ä¼šé‡è¯•")
                                _set_upload_progress(record_id, {
                                    'status': 'error',
                                    'stage': 'å¤±è´¥',
                                    'percent': 100,
                                    'message': error_msg
                                })

                        else:
                            error_msg = f"ä¸Šä¼ èƒ¶å›Š {record_id} å¤±è´¥: result is None"
                            errors.append(error_msg)
                            print(f"   âœ— {error_msg}")
                            _set_upload_progress(record_id, {
                                'status': 'error',
                                'stage': 'å¤±è´¥',
                                'percent': 100,
                                'message': error_msg
                            })

                    except Exception as e:
                        error_msg = f"ä¸Šä¼ èƒ¶å›Š {record['record_id']} å¤±è´¥: {e}"
                        errors.append(error_msg)
                        print(f"   âœ— {error_msg}")
                        import traceback
                        print(traceback.format_exc())
                        _set_upload_progress(record_id, {
                            'status': 'error',
                            'stage': 'å¤±è´¥',
                            'percent': 100,
                            'message': error_msg
                        })
            else:
                print("   âœ“ æ— å¾…ä¸Šä¼ çš„å…ƒæ•°æ®")

            print()

            # 2. ä¸‹è½½äº‘ç«¯å˜æ›´ï¼ˆå…ƒæ•°æ®ï¼‰
            print("ğŸ“¥ æ­¥éª¤ 2: ä¸‹è½½å…¨çƒèƒ¶å›Šå…ƒæ•°æ®...")
            print("   [GLOBAL SYNC] æ‹‰å–æ‰€æœ‰ç”¨æˆ·çš„èƒ¶å›Šï¼ˆä»…å…ƒæ•°æ®ï¼‰")
            supabase = get_supabase_client()
            if supabase:
                # è·å–äº‘ç«¯æ‰€æœ‰èƒ¶å›Šçš„å…ƒæ•°æ®ï¼ˆPhase G: å…¨çƒåŒæ­¥ï¼‰
                cloud_capsules = supabase.download_capsules(user_id)

                if cloud_capsules:
                    print(f"   [GLOBAL SYNC] å‘ç° {len(cloud_capsules)} ä¸ªå…¨çƒèƒ¶å›Š")

                    # ç»Ÿè®¡ä¸åŒç”¨æˆ·çš„èƒ¶å›Š
                    user_stats = {}
                    for cap in cloud_capsules:
                        uid = cap.get('user_id', 'unknown')
                        user_stats[uid] = user_stats.get(uid, 0) + 1

                    print(f"   [GLOBAL SYNC] ç”¨æˆ·åˆ†å¸ƒ: {user_stats}")

                    for cloud_capsule in cloud_capsules:
                        try:
                            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨
                            local_capsule = self._get_local_capsule_by_cloud_id(cloud_capsule['id'])

                            if local_capsule:
                                # æ›´æ–°æœ¬åœ°å…ƒæ•°æ®
                                self._update_local_capsule_metadata(local_capsule['id'], cloud_capsule)
                                owner_id = cloud_capsule.get('user_id', 'unknown')[:8]
                                print(f"   âœ“ æ›´æ–°èƒ¶å›Š {cloud_capsule['name']} (by {owner_id}...)")
                            else:
                                # åˆ›å»ºæ–°èƒ¶å›Šï¼ˆä»…å…ƒæ•°æ®ï¼‰
                                new_capsule_id = self._create_local_capsule_from_cloud(cloud_capsule)
                                owner_id = cloud_capsule.get('user_id', 'unknown')[:8]
                                print(f"   âœ“ æ–°å¢èƒ¶å›Š {cloud_capsule['name']} (by {owner_id}...)")
                                synced_count += 1

                        except Exception as e:
                            error_msg = f"åŒæ­¥èƒ¶å›Š {cloud_capsule.get('name', 'Unknown')} å¤±è´¥: {e}"
                            errors.append(error_msg)
                            print(f"   âœ— {error_msg}")
                else:
                    print("   [GLOBAL SYNC] äº‘ç«¯æš‚æ— èƒ¶å›Šæ•°æ®")
            else:
                print("   âš ï¸  Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡äº‘ç«¯ä¸‹è½½")

            print()

            # 3. ä¸‹è½½è½»é‡èµ„äº§æ–‡ä»¶ï¼ˆOGG é¢„è§ˆ + RPP é¡¹ç›®æ–‡ä»¶ï¼‰
            logger.info("ğŸ“¥ æ­¥éª¤ 3: ä¸‹è½½è½»é‡èµ„äº§æ–‡ä»¶ï¼ˆOGG + RPPï¼‰...")

            # è·å–æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„æœ¬åœ°èƒ¶å›Šï¼ˆåŒ…æ‹¬æ–°å¢å’Œæ›´æ–°çš„ï¼‰
            from capsule_db import get_database
            db = get_database()
            db.connect()
            cursor = db.conn.cursor()

            cursor.execute("""
                SELECT id, name, uuid, preview_audio, cloud_status, asset_status,
                       owner_supabase_user_id, cloud_id, file_path
                FROM capsules
                WHERE cloud_id IS NOT NULL
                ORDER BY id
            """)
            local_capsules = cursor.fetchall()

            db.close()

            logger.info(f"   æŸ¥è¯¢åˆ° {len(local_capsules)} ä¸ªèƒ¶å›Šéœ€è¦æ£€æŸ¥è½»é‡èµ„äº§")

            if local_capsules:
                logger.info(f"   æ£€æŸ¥ {len(local_capsules)} ä¸ªèƒ¶å›Šçš„è½»é‡èµ„äº§...")

                for idx, cap in enumerate(local_capsules, 1):
                    cap_id, cap_name, cap_uuid, preview_audio, cloud_status, asset_status, owner_id, cloud_id, cap_file_path = cap

                    logger.info(f"   [{idx}/{len(local_capsules)}] æ£€æŸ¥èƒ¶å›Š: {cap_name}, owner_id: {owner_id}")

                    # å‡†å¤‡æœ¬åœ°è·¯å¾„ - ä» PathManager è·å–å¯¼å‡ºç›®å½•
                    from common import PathManager
                    pm = PathManager.get_instance()
                    export_dir = pm.export_dir
                    logger.info(f"   ä½¿ç”¨å¯¼å‡ºç›®å½•: {export_dir}")

                    capsule_rel_path = cap_file_path or cap_name
                    capsule_dir = Path(export_dir) / capsule_rel_path

                    # âœ… çŠ¶æ€è‡ªæ„ˆï¼šå¦‚æœæœ¬åœ°æœ‰ Audio æ–‡ä»¶å¤¹ï¼Œæ›´æ–° asset_status
                    if self._has_local_audio_files(capsule_dir):
                        if self._update_asset_status_if_needed(cap_id, asset_status, 'local'):
                            logger.info(f"   âœ¨ æ£€æµ‹åˆ°æœ¬åœ°éŸ³é¢‘ï¼Œä¿®æ­£èµ„äº§çŠ¶æ€: {cap_name} -> local")
                        asset_status = 'local'
                    needs_download = []
                    current_file = ''

                    try:
                        # æ£€æŸ¥ metadata.json æ–‡ä»¶
                        metadata_path = capsule_dir / "metadata.json"
                        if not metadata_path.exists():
                            needs_download.append(('metadata', 'metadata.json'))
                            current_file = f"{cap_name}/metadata.json"
                            logger.info(f"      - éœ€è¦ä¸‹è½½å…ƒæ•°æ®: metadata.json")
                        
                        # æ£€æŸ¥ OGG é¢„è§ˆæ–‡ä»¶
                        if preview_audio:
                            ogg_path = capsule_dir / preview_audio
                            if not ogg_path.exists():
                                needs_download.append(('preview', preview_audio))
                                if not current_file:
                                    current_file = f"{cap_name}/preview.{preview_audio.split('.')[-1]}"
                                logger.info(f"      - éœ€è¦ä¸‹è½½é¢„è§ˆéŸ³é¢‘: {preview_audio}")

                        # æ£€æŸ¥ RPP é¡¹ç›®æ–‡ä»¶ï¼ˆä½¿ç”¨èƒ¶å›Šåç§°ï¼‰
                        rpp_filename = f"{cap_name}.rpp"
                        rpp_path = capsule_dir / rpp_filename
                        if not rpp_path.exists():
                            needs_download.append(('rpp', rpp_filename))
                            if not current_file:
                                current_file = f"{cap_name}/{rpp_filename}"
                            logger.info(f"      - éœ€è¦ä¸‹è½½é¡¹ç›®æ–‡ä»¶: {rpp_filename}")

                        if not needs_download:
                            logger.info(f"      âœ“ æ‰€æœ‰è½»é‡èµ„äº§å·²å­˜åœ¨")

                        # ä¸‹è½½ç¼ºå¤±çš„æ–‡ä»¶
                        if needs_download and owner_id:
                            supabase = get_supabase_client()
                            if not supabase:
                                logger.warning(f"   âš ï¸  Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ–‡ä»¶ä¸‹è½½")
                                break

                            for file_type, filename in needs_download:
                                try:
                                    logger.info(f"   [{idx}/{len(local_capsules)}] æ­£åœ¨ä¸‹è½½ {filename}...")

                                    # æ„å»ºæœ¬åœ°è·¯å¾„
                                    local_path = capsule_dir / filename

                                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                                    capsule_dir.mkdir(parents=True, exist_ok=True)

                                    # è°ƒç”¨ä¸‹è½½
                                    # æ³¨æ„ï¼šäº‘ç«¯æ–‡ä»¶å¤¹ä½¿ç”¨èƒ¶å›Šåç§°ï¼Œè€Œä¸æ˜¯ uuid
                                    success = supabase.download_file(
                                        user_id=owner_id,
                                        capsule_folder_name=cap_name,
                                        file_type=file_type,
                                        local_path=str(local_path)
                                    )

                                    if success:
                                        if file_type == 'preview':
                                            preview_downloaded += 1
                                        logger.info(f"   âœ“ [{idx}/{len(local_capsules)}] {filename} ä¸‹è½½æˆåŠŸ")
                                    else:
                                        logger.error(f"   âœ— [{idx}/{len(local_capsules)}] {filename} ä¸‹è½½å¤±è´¥")

                                except Exception as e:
                                    error_msg = f"ä¸‹è½½ {cap_name}/{filename} å¤±è´¥: {e}"
                                    errors.append(error_msg)
                                    logger.error(f"   âœ— {error_msg}")
                        
                        # ğŸ·ï¸ å¤„ç† Tagsï¼šä¼˜å…ˆä½¿ç”¨äº‘ç«¯æ•°æ®åº“ï¼Œæ–‡ä»¶ä½œä¸ºå¤‡ä»½
                        try:
                            from tags_service import get_tags_service
                            tags_service = get_tags_service()
                            
                            # ä½¿ç”¨å·²è§£åŒ…çš„ cloud_id å˜é‡
                            if cloud_id and supabase:
                                # å°è¯•ä»äº‘ç«¯æ•°æ®åº“æ‹‰å– Tags
                                logger.info(f"   ğŸ·ï¸  å°è¯•ä»äº‘ç«¯æ•°æ®åº“æ‹‰å– Tags...")
                                tags_synced = tags_service.sync_tags_from_cloud(cap_id, cloud_id)
                                
                                if tags_synced:
                                    logger.info(f"   âœ“ Tags å·²ä»äº‘ç«¯åŒæ­¥")
                                else:
                                    # å¦‚æœäº‘ç«¯æ²¡æœ‰ Tagsï¼Œå°è¯•ä» metadata.json å¯¼å…¥
                                    metadata_path = capsule_dir / "metadata.json"
                                    if metadata_path.exists():
                                        logger.info(f"   âš ï¸  äº‘ç«¯æ—  Tagsï¼Œå°è¯•ä» metadata.json å¯¼å…¥...")
                                        tags_service.merge_tags_from_metadata(cap_id, metadata_path)
                            else:
                                # ç¦»çº¿æ¨¡å¼ï¼šä» metadata.json å¯¼å…¥
                                metadata_path = capsule_dir / "metadata.json"
                                if metadata_path.exists():
                                    logger.info(f"   â„¹ï¸  ç¦»çº¿æ¨¡å¼ï¼Œä» metadata.json å¯¼å…¥ Tags...")
                                    tags_service.merge_tags_from_metadata(cap_id, metadata_path)
                        except Exception as e:
                            logger.warning(f"   âš ï¸  Tags å¤„ç†å¤±è´¥: {e}")

                        # ğŸ“Š å¤„ç†æŠ€æœ¯å…ƒæ•°æ®ï¼šä» metadata.json å†™å…¥ capsule_metadata è¡¨
                        try:
                            metadata_path = capsule_dir / "metadata.json"
                            if metadata_path.exists():
                                self._save_metadata_to_db(cap_id, metadata_path)
                        except Exception as e:
                            logger.warning(f"   âš ï¸  å…ƒæ•°æ®å†™å…¥å¤±è´¥: {e}")

                    except Exception as e:
                        error_msg = f"æ£€æŸ¥ {cap_name} èµ„äº§å¤±è´¥: {e}"
                        errors.append(error_msg)
                        logger.error(f"   âœ— {error_msg}")

                logger.info(f"   âœ“ é¢„è§ˆéŸ³é¢‘ä¸‹è½½: {preview_downloaded} ä¸ª")

            else:
                logger.info("   âœ“ æ— éœ€ä¸‹è½½èµ„äº§ï¼ˆæœ¬åœ°æš‚æ— èƒ¶å›Šï¼‰")

            # 4. ä¸è‡ªåŠ¨ä¸‹è½½æº WAVï¼ˆæŒ‰éœ€ä¸‹è½½ï¼‰
            print("ğŸ“¥ æ­¥éª¤ 4: æº WAV æ–‡ä»¶")
            print("   â„¹ï¸  æº WAV æ–‡ä»¶é‡‡ç”¨æŒ‰éœ€ä¸‹è½½ç­–ç•¥")
            print("   â„¹ï¸  ç”¨æˆ·ç‚¹å‡»\"å¯¼å…¥\"æ—¶æ‰ä¼šä¸‹è½½ WAV")
            print()

        except Exception as e:
            error_msg = f"åŒæ­¥è¿‡ç¨‹å‡ºé”™: {e}"
            errors.append(error_msg)
            print(f"âŒ {error_msg}")

        # è®¡ç®—è€—æ—¶
        duration = time.time() - start_time

        # æ‰“å°æ€»ç»“
        print("=" * 60)
        print("ğŸ“Š åŒæ­¥å®Œæˆ")
        print("=" * 60)
        print(f"åŒæ­¥èƒ¶å›Šæ•°: {synced_count}")
        print(f"é¢„è§ˆéŸ³é¢‘ä¸‹è½½: {preview_downloaded}")
        print(f"é”™è¯¯æ•°é‡: {len(errors)}")
        print(f"è€—æ—¶: {duration:.2f} ç§’")

        if errors:
            print()
            print("é”™è¯¯è¯¦æƒ…:")
            for error in errors:
                print(f"  â€¢ {error}")

        print("=" * 60)
        print()

        return {
            'success': len(errors) == 0,
            'synced_count': synced_count,
            'preview_downloaded': preview_downloaded,
            'errors': errors,
            'duration_seconds': duration
        }

    def _get_capsule_metadata_only(self, capsule_id: int) -> Dict[str, Any]:
        """
        è·å–èƒ¶å›Šå…ƒæ•°æ®ï¼ˆä¸å« WAV æ–‡ä»¶ï¼‰

        Args:
            capsule_id: èƒ¶å›Š ID

        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT
                    id, name, capsule_type, keywords, description,
                    created_at, updated_at, cloud_id,
                    cloud_status, asset_access_count,
                    file_path, preview_audio, rpp_file, owner_supabase_user_id
                FROM capsules
                WHERE id = ?
            """, (capsule_id,))

            row = cursor.fetchone()
            if not row:
                raise ValueError(f"èƒ¶å›Š {capsule_id} ä¸å­˜åœ¨")

            return dict(row)

        finally:
            conn.close()

    def _get_local_capsule_by_cloud_id(self, cloud_id: str) -> Optional[Dict]:
        """
        æ ¹æ® cloud_id æŸ¥æ‰¾æœ¬åœ°èƒ¶å›Š

        Args:
            cloud_id: äº‘ç«¯èƒ¶å›Š ID (Supabase record ID)

        Returns:
            æœ¬åœ°èƒ¶å›Šå­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, name, cloud_status, asset_status, owner_supabase_user_id
                FROM capsules
                WHERE cloud_id = ?
            """, (cloud_id,))

            row = cursor.fetchone()
            return dict(row) if row else None

        finally:
            conn.close()

    def _get_local_capsule_by_name(self, name: str) -> Optional[Dict]:
        """
        æ ¹æ®åç§°æŸ¥æ‰¾æœ¬åœ°èƒ¶å›Šï¼ˆç”¨äºåŒ¹é…æœ¬åœ°æ‰«æåˆ›å»ºçš„èƒ¶å›Šï¼‰
        
        Args:
            name: èƒ¶å›Šåç§°
            
        Returns:
            æœ¬åœ°èƒ¶å›Šå­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, name, cloud_status, asset_status, owner_supabase_user_id, cloud_id
                FROM capsules
                WHERE name = ?
            """, (name,))

            row = cursor.fetchone()
            return dict(row) if row else None

        finally:
            conn.close()

    def _set_capsule_cloud_id(self, local_id: int, cloud_id: str) -> bool:
        """
        è®¾ç½®èƒ¶å›Šçš„ cloud_idï¼ˆå…³è”æœ¬åœ°æ‰«æçš„èƒ¶å›Šä¸äº‘ç«¯è®°å½•ï¼‰
        
        Args:
            local_id: æœ¬åœ°èƒ¶å›Š ID
            cloud_id: äº‘ç«¯èƒ¶å›Š ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            # ğŸ”¥ åŒæ—¶è®¾ç½® audio_uploaded = 1ï¼Œå› ä¸ºäº‘ç«¯å·²æœ‰å®Œæ•´æ•°æ®
            cursor.execute("""
                UPDATE capsules
                SET cloud_id = ?, cloud_status = 'synced', audio_uploaded = 1
                WHERE id = ?
            """, (cloud_id, local_id))
            conn.commit()
            return True
        except Exception as e:
            conn.rollback()
            print(f"âŒ è®¾ç½® cloud_id å¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def _update_local_capsule_metadata(self, local_id: int, cloud_data: Dict) -> bool:
        """
        æ›´æ–°æœ¬åœ°èƒ¶å›Šå…ƒæ•°æ®ï¼ˆä¸è¦†ç›– asset_statusï¼‰

        Phase G: æ·»åŠ  owner_supabase_user_id æ›´æ–°
        Phase G2: æ·»åŠ  preview_audioã€keywordsã€description ç­‰å­—æ®µä» metadata æå–

        Args:
            local_id: æœ¬åœ°èƒ¶å›Š ID
            cloud_data: äº‘ç«¯æ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # å…ˆè·å–æœ¬åœ°ç°æœ‰çš„ preview_audio å€¼ï¼ˆé¿å…è¢«äº‘ç«¯ç©ºå€¼è¦†ç›–ï¼‰
            cursor.execute("SELECT preview_audio FROM capsules WHERE id = ?", (local_id,))
            local_row = cursor.fetchone()
            local_preview_audio = local_row['preview_audio'] if local_row else None

            # ä» metadata ä¸­æå–å®Œæ•´å…ƒæ•°æ®
            metadata = cloud_data.get('metadata', {})
            if isinstance(metadata, dict):
                cloud_preview_audio = metadata.get('preview_audio')
                keywords = metadata.get('keywords')
                description = metadata.get('description')
                capsule_type = metadata.get('capsule_type', cloud_data.get('capsule_type', 'magic'))
            else:
                cloud_preview_audio = None
                keywords = cloud_data.get('keywords')
                description = cloud_data.get('description')
                capsule_type = cloud_data.get('capsule_type', 'magic')

            # âš ï¸ é‡è¦ï¼šåªæœ‰äº‘ç«¯æœ‰å€¼æ—¶æ‰è¦†ç›–æœ¬åœ°ï¼Œå¦åˆ™ä¿ç•™æœ¬åœ°å€¼
            preview_audio = cloud_preview_audio if cloud_preview_audio else local_preview_audio

            # åªæ›´æ–°å…ƒæ•°æ®å­—æ®µï¼Œä¿ç•™ asset_status å’Œ cloud_status
            # âš ï¸ é‡è¦ï¼šä¸è‡ªåŠ¨è¦†ç›– cloud_statusï¼
            # å¦‚æœæœ¬åœ°çŠ¶æ€æ˜¯ 'local'ï¼ˆéœ€è¦ä¸Šä¼ ï¼‰ï¼Œä¿æŒä¸å˜
            # åªæœ‰ä¸Šä¼ æˆåŠŸåæ‰é€šè¿‡ mark_as_synced() æ”¹ä¸º 'synced'
            cursor.execute("""
                UPDATE capsules
                SET
                    name = ?,
                    capsule_type = ?,
                    keywords = ?,
                    description = ?,
                    preview_audio = ?,
                    owner_supabase_user_id = ?,
                    updated_at = ?
                WHERE id = ?
            """, (
                cloud_data.get('name'),
                capsule_type,
                keywords,
                description,
                preview_audio,  # ä¼˜å…ˆä½¿ç”¨äº‘ç«¯å€¼ï¼Œå¦åˆ™ä¿ç•™æœ¬åœ°å€¼
                cloud_data.get('user_id'),  # Phase G: æ›´æ–°æ‰€æœ‰è€… ID
                datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                local_id
            ))

            conn.commit()
            return True

        except Exception as e:
            conn.rollback()
            print(f"âŒ æ›´æ–°æœ¬åœ°èƒ¶å›Šå¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def _create_local_capsule_from_cloud(self, cloud_data: Dict) -> int:
        """
        ä»äº‘ç«¯æ•°æ®åˆ›å»ºæœ¬åœ°èƒ¶å›Šï¼ˆä»…å…ƒæ•°æ®ï¼‰

        Phase G: æ·»åŠ  owner_supabase_user_id å­—æ®µä»¥æ”¯æŒå¤šç”¨æˆ·å…±äº«
        Phase G2: æ·»åŠ  preview_audioã€keywordsã€description ç­‰å­—æ®µä» metadata æå–

        Args:
            cloud_data: äº‘ç«¯æ•°æ®

        Returns:
            æ–°åˆ›å»ºçš„æœ¬åœ°èƒ¶å›Š ID
        """
        conn = self._get_connection()
        try:
            cursor = conn.cursor()

            # ä» metadata ä¸­æå–å®Œæ•´å…ƒæ•°æ®
            metadata = cloud_data.get('metadata', {})
            if isinstance(metadata, dict):
                preview_audio = metadata.get('preview_audio')
                keywords = metadata.get('keywords')
                description = metadata.get('description')
                capsule_type = metadata.get('capsule_type', cloud_data.get('capsule_type', 'magic'))
            else:
                preview_audio = None
                keywords = cloud_data.get('keywords')
                description = cloud_data.get('description')
                capsule_type = cloud_data.get('capsule_type', 'magic')

            cloud_uuid = cloud_data.get('uuid', str(cloud_data.get('id')))
            cloud_name = cloud_data.get('name')
            cloud_file_path = cloud_data.get('file_path', cloud_name)
            cloud_id = cloud_data.get('id')

            # 1) ä¼˜å…ˆæŒ‰ cloud_id åŒ¹é…ï¼ˆé˜²æ­¢é‡å¤æ’å…¥ï¼‰
            if cloud_id:
                cursor.execute("""
                    SELECT id FROM capsules WHERE cloud_id = ?
                """, (cloud_id,))
                existing = cursor.fetchone()
                if existing:
                    existing_id = existing['id'] if isinstance(existing, sqlite3.Row) else existing[0]
                    self._update_local_capsule_metadata(existing_id, cloud_data)
                    conn.commit()
                    return existing_id

            # 2) æŒ‰ uuid åŒ¹é…ï¼ˆuuid ä¸ºç©ºæ—¶è·³è¿‡ï¼‰
            if cloud_uuid:
                cursor.execute("""
                    SELECT id FROM capsules WHERE uuid = ?
                """, (cloud_uuid,))
                existing = cursor.fetchone()
                if existing:
                    existing_id = existing['id'] if isinstance(existing, sqlite3.Row) else existing[0]
                    cursor.execute("""
                        UPDATE capsules
                        SET
                            name = ?,
                            capsule_type = ?,
                            keywords = ?,
                            description = ?,
                            preview_audio = ?,
                            file_path = ?,
                            cloud_id = ?,
                            cloud_status = 'synced',
                            owner_supabase_user_id = ?,
                            updated_at = ?
                        WHERE id = ?
                    """, (
                        cloud_name,
                        capsule_type,
                        keywords,
                        description,
                        preview_audio,
                        cloud_file_path,
                        cloud_id,
                        cloud_data.get('user_id'),
                        datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                        existing_id
                    ))
                    conn.commit()
                    return existing_id

            # 3) å…œåº•ï¼šæŒ‰ name + file_path åŒ¹é…ï¼ˆå¤„ç†æœ¬åœ°æ‰«æå·²å­˜åœ¨çš„èƒ¶å›Šï¼‰
            if cloud_name:
                cursor.execute("""
                    SELECT id FROM capsules
                    WHERE name = ? AND file_path = ?
                    LIMIT 1
                """, (cloud_name, cloud_file_path))
                existing = cursor.fetchone()
                if existing:
                    existing_id = existing['id'] if isinstance(existing, sqlite3.Row) else existing[0]
                    cursor.execute("""
                        UPDATE capsules
                        SET
                            uuid = COALESCE(uuid, ?),
                            capsule_type = ?,
                            keywords = ?,
                            description = ?,
                            preview_audio = ?,
                            cloud_id = ?,
                            cloud_status = 'synced',
                            owner_supabase_user_id = ?,
                            updated_at = ?
                        WHERE id = ?
                    """, (
                        cloud_uuid,
                        capsule_type,
                        keywords,
                        description,
                        preview_audio,
                        cloud_id,
                        cloud_data.get('user_id'),
                        datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
                        existing_id
                    ))
                    conn.commit()
                    return existing_id

            # æ’å…¥æ–°èƒ¶å›Š
            # rpp_file ä½¿ç”¨é»˜è®¤å‘½åè§„åˆ™ï¼š{capsule_name}.rpp
            rpp_file = f"{cloud_name}.rpp" if cloud_name else None
            
            # ğŸ”¥ æ£€æµ‹æœ¬åœ°æ˜¯å¦å·²æœ‰æ–‡ä»¶ï¼ŒåŠ¨æ€è®¾ç½® asset_status
            asset_status = 'cloud_only'  # é»˜è®¤
            try:
                from common import PathManager
                pm = PathManager.get_instance()
                export_dir = pm.export_dir
                capsule_dir = Path(export_dir) / cloud_file_path
                
                # æ£€æµ‹æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if capsule_dir.exists():
                    # æ£€æŸ¥æ˜¯å¦æœ‰ Audio æ–‡ä»¶å¤¹ï¼ˆå®Œæ•´èµ„äº§ï¼‰
                    audio_dir = capsule_dir / "Audio"
                    if audio_dir.exists() and list(audio_dir.glob("*.wav")):
                        asset_status = 'local'
                        logger.info(f"   â„¹ï¸ æ£€æµ‹åˆ°æœ¬åœ°å®Œæ•´èµ„äº§: {cloud_name} -> local")
                    # åªæœ‰é¢„è§ˆæ–‡ä»¶ï¼ˆOGGï¼‰ä¿æŒ cloud_onlyï¼Œä¸æ”¹ä¸º local
                    # å› ä¸º local æ„å‘³ç€æœ‰å®Œæ•´çš„ Audio/WAV æ–‡ä»¶
            except Exception as e:
                logger.warning(f"   âš ï¸ æ£€æµ‹æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
            
            cursor.execute("""
                INSERT INTO capsules (
                    uuid, name, capsule_type, keywords, description, preview_audio, file_path,
                    rpp_file,
                    cloud_id, cloud_status, asset_status, audio_uploaded,
                    owner_supabase_user_id,
                    created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'synced', ?, 1, ?, ?, ?)
            """, (
                cloud_uuid,  # ä½¿ç”¨äº‘ç«¯ ID ä½œä¸º uuid
                cloud_name,
                capsule_type,
                keywords,
                description,
                preview_audio,  # Phase G2: æ·»åŠ é¢„è§ˆéŸ³é¢‘æ–‡ä»¶å
                cloud_file_path,  # æ–‡ä»¶è·¯å¾„é»˜è®¤ä¸º name
                rpp_file,  # ğŸ”¥ æ·»åŠ  RPP æ–‡ä»¶å
                cloud_id,  # cloud_id (Supabase record ID)
                asset_status,  # ğŸ”¥ åŠ¨æ€æ£€æµ‹çš„ asset_status
                # ğŸ”¥ audio_uploaded = 1ï¼Œå› ä¸ºä»äº‘ç«¯åŒæ­¥çš„èƒ¶å›Šï¼ŒAudio å·²åœ¨äº‘ç«¯
                cloud_data.get('user_id'),  # Phase G: ä¿å­˜æ‰€æœ‰è€… ID
                cloud_data.get('created_at'),
                datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
            ))

            new_id = cursor.lastrowid
            conn.commit()

            return new_id

        except Exception as e:
            conn.rollback()
            print(f"âŒ åˆ›å»ºæœ¬åœ°èƒ¶å›Šå¤±è´¥: {e}")
            raise
        finally:
            conn.close()


    # ========== Phase C1: æ£±é•œé…ç½®åŒæ­¥ ==========

    def sync_prisms(self, user_id: str) -> Dict[str, Any]:
        """
        åŒæ­¥æ£±é•œé…ç½®åˆ°äº‘ç«¯

        Phase C1: æ£±é•œç‰ˆæœ¬æ§åˆ¶

        ç­–ç•¥: Last Write Wins
        - ä¸Šä¼ æœ¬åœ°å˜æ›´ï¼ˆversion > äº‘ç«¯ versionï¼‰
        - ä¸‹è½½äº‘ç«¯å˜æ›´ï¼ˆåº”ç”¨ Last Write Winsï¼‰
        - å†²çªè‡ªåŠ¨è§£å†³ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„

        Args:
            user_id: Supabase ç”¨æˆ· ID

        Returns:
            åŒæ­¥ç»“æœï¼š{
                'success': bool,
                'uploaded': int,
                'downloaded': int,
                'conflicts_resolved': int,
                'errors': List[str]
            }
        """
        from prism_version_manager import PrismVersionManager
        from dal_cloud_prisms import get_cloud_prism_dal

        errors = []
        uploaded = 0
        downloaded = 0
        conflicts_resolved = 0

        print("=" * 60)
        print("ğŸ”„ æ£±é•œé…ç½®åŒæ­¥")
        print("=" * 60)
        print(f"ç”¨æˆ· ID: {user_id}")
        print()

        try:
            # åˆå§‹åŒ–ç®¡ç†å™¨å’Œ DAL
            prism_manager = PrismVersionManager(self.db_path)
            prism_dal = get_cloud_prism_dal()

            # åŠ è½½ anchor_configï¼ˆç”¨äºä¸Šä¼ æ—¶å¸¦ä¸Š is_activeï¼Œä¸‹è½½åå†™å› is_activeï¼‰
            anchor_config = {}
            anchor_config_path = None
            try:
                from common import PathManager
                pm = PathManager.get_instance()
                anchor_config_path = pm.config_dir / "anchor_config_v2.json"
                if anchor_config_path.exists():
                    with open(anchor_config_path, 'r', encoding='utf-8') as f:
                        anchor_config = json.load(f)
            except Exception:
                try:
                    p = Path(__file__).parent / "anchor_config_v2.json"
                    if p.exists():
                        anchor_config_path = p
                        with open(p, 'r', encoding='utf-8') as f:
                            anchor_config = json.load(f)
                except Exception:
                    pass

            # 1. ä¸Šä¼ æœ¬åœ°å˜æ›´
            print("ğŸ“¤ æ­¥éª¤ 1: ä¸Šä¼ æœ¬åœ°æ£±é•œå˜æ›´...")
            dirty_prisms = prism_manager.get_dirty_prisms()

            if dirty_prisms:
                print(f"   å‘ç° {len(dirty_prisms)} ä¸ªæœ¬åœ°å˜æ›´")

                for prism in dirty_prisms:
                    try:
                        # æ£±é•œå¯ç”¨çŠ¶æ€ä» anchor_config æ³¨å…¥ï¼Œä¾›äº‘ç«¯åŒæ­¥
                        prism['is_active'] = anchor_config.get(prism['id'], {}).get('active', True)
                        # ä½¿ç”¨ DAL ä¸Šä¼ ï¼ˆå« field_dataã€is_activeï¼‰
                        result = prism_dal.upload_prism(
                            user_id,
                            prism['id'],
                            prism  # ç›´æ¥ä¼ é€’å®Œæ•´çš„ prism å­—å…¸
                        )

                        if result:
                            uploaded += 1
                            print(f"   âœ… ä¸Šä¼ æ£±é•œ '{prism['id']}' (v{prism['version']})")
                        else:
                            errors.append(f"ä¸Šä¼ æ£±é•œ '{prism['id']}' å¤±è´¥")

                    except Exception as e:
                        error_msg = f"ä¸Šä¼ æ£±é•œ '{prism['id']}' å¤±è´¥: {e}"
                        errors.append(error_msg)
                        print(f"   âŒ {error_msg}")
            else:
                print("   âœ… æ— æœ¬åœ°å˜æ›´éœ€è¦ä¸Šä¼ ")

            print()

            # 2. ä¸‹è½½äº‘ç«¯å˜æ›´
            print("ğŸ“¥ æ­¥éª¤ 2: ä¸‹è½½äº‘ç«¯æ£±é•œå˜æ›´...")

            try:
                # é€šè¿‡ DAL è·å–äº‘ç«¯æ‰€æœ‰æ£±é•œ
                cloud_prisms = prism_dal.download_prisms(user_id)

                if cloud_prisms:
                    print(f"   å‘ç° {len(cloud_prisms)} ä¸ªäº‘ç«¯æ£±é•œ")
                    # åœ¨ç°æœ‰ anchor_config ä¸Šåªæ›´æ–°å„æ£±é•œçš„ activeï¼ˆä¿ç•™ name/axes ç­‰ï¼‰
                    anchor_config_to_save = dict(anchor_config) if anchor_config else {}

                    for cloud_prism in cloud_prisms:
                        try:
                            prism_id = cloud_prism['prism_id']
                            # æ£±é•œå…³é”®è¯ field_dataï¼ˆäº‘ç«¯ JSON å­—ç¬¦ä¸² â†’ è§£æä¸º listï¼‰
                            raw_field = cloud_prism.get('field_data')
                            field_data = json.loads(raw_field) if isinstance(raw_field, str) and raw_field else (raw_field if isinstance(raw_field, list) else [])
                            # æ£±é•œå¯ç”¨çŠ¶æ€
                            is_active = cloud_prism.get('is_active')
                            if is_active is None:
                                is_active = True

                            # æ£€æŸ¥æœ¬åœ°ç‰ˆæœ¬
                            local_prism = prism_manager.get_prism(prism_id)

                            if local_prism:
                                # ç‰ˆæœ¬æ¯”è¾ƒ
                                local_version = local_prism['version']
                                cloud_version = cloud_prism['version']

                                if cloud_version > local_version:
                                    # äº‘ç«¯ç‰ˆæœ¬æ›´æ–°ï¼Œåº”ç”¨äº‘ç«¯é…ç½®ï¼ˆå« field_dataã€Last Write Winsï¼‰
                                    prism_data = {
                                        'name': cloud_prism['name'],
                                        'description': cloud_prism['description'],
                                        'axis_config': json.loads(cloud_prism['axis_config']) if isinstance(cloud_prism.get('axis_config'), str) else (cloud_prism.get('axis_config') or {}),
                                        'anchors': json.loads(cloud_prism['anchors']) if isinstance(cloud_prism.get('anchors'), str) else (cloud_prism.get('anchors') or []),
                                        'field_data': field_data,
                                    }

                                    prism_manager.create_or_update_prism(
                                        prism_id,
                                        prism_data,
                                        user_id='cloud_sync'
                                    )
                                    if prism_id not in anchor_config_to_save:
                                        anchor_config_to_save[prism_id] = {}
                                    anchor_config_to_save[prism_id]['active'] = is_active

                                    downloaded += 1
                                    conflicts_resolved += 1
                                    print(f"   âœ… ä¸‹è½½æ£±é•œ '{prism_id}' (v{local_version} â†’ v{cloud_version})")

                                elif cloud_version < local_version:
                                    # æœ¬åœ°ç‰ˆæœ¬æ›´æ–°ï¼Œå·²åœ¨æ­¥éª¤1ä¸Šä¼ ï¼›ä»åº”ç”¨äº‘ç«¯ is_active
                                    if prism_id not in anchor_config_to_save:
                                        anchor_config_to_save[prism_id] = {}
                                    anchor_config_to_save[prism_id]['active'] = is_active
                                    print(f"   â„¹ï¸  æ£±é•œ '{prism_id}' æœ¬åœ°ç‰ˆæœ¬æ›´æ–° (v{local_version} > v{cloud_version})")
                                else:
                                    # ç‰ˆæœ¬ç›¸åŒï¼Œä»åº”ç”¨äº‘ç«¯ is_active åˆ°æœ¬åœ°é…ç½®
                                    if prism_id not in anchor_config_to_save:
                                        anchor_config_to_save[prism_id] = {}
                                    anchor_config_to_save[prism_id]['active'] = is_active
                                    print(f"   â„¹ï¸  æ£±é•œ '{prism_id}' ç‰ˆæœ¬ä¸€è‡´ (v{local_version})")
                            else:
                                # æœ¬åœ°ä¸å­˜åœ¨ï¼Œç›´æ¥åˆ›å»ºï¼ˆå« field_dataï¼‰
                                prism_data = {
                                    'name': cloud_prism['name'],
                                    'description': cloud_prism['description'],
                                    'axis_config': json.loads(cloud_prism['axis_config']) if isinstance(cloud_prism.get('axis_config'), str) else (cloud_prism.get('axis_config') or {}),
                                    'anchors': json.loads(cloud_prism['anchors']) if isinstance(cloud_prism.get('anchors'), str) else (cloud_prism.get('anchors') or []),
                                    'field_data': field_data,
                                }

                                prism_manager.create_or_update_prism(
                                    prism_id,
                                    prism_data,
                                    user_id='cloud_sync'
                                )
                                if prism_id not in anchor_config_to_save:
                                    anchor_config_to_save[prism_id] = {}
                                anchor_config_to_save[prism_id]['active'] = is_active

                                downloaded += 1
                                print(f"   âœ… ä¸‹è½½æ–°æ£±é•œ '{prism_id}' (v{cloud_prism['version']})")

                        except Exception as e:
                            error_msg = f"å¤„ç†æ£±é•œ '{cloud_prism['prism_id']}' å¤±è´¥: {e}"
                            errors.append(error_msg)
                            print(f"   âŒ {error_msg}")

                    # ä»¥äº‘ç«¯ä¸ºå‡†ï¼šåˆ é™¤æœ¬åœ°å­˜åœ¨ä½†äº‘ç«¯ä¸å­˜åœ¨çš„æ£±é•œï¼ˆå¦‚æ—§æµ‹è¯•æ£±é•œ mechanicsã€force_field_testï¼‰
                    cloud_ids = [p['prism_id'] for p in cloud_prisms]
                    try:
                        conn = sqlite3.connect(self.db_path)
                        try:
                            cursor = conn.cursor()
                            placeholders = ','.join('?' * len(cloud_ids))
                            cursor.execute(f"DELETE FROM prism_versions WHERE prism_id NOT IN ({placeholders})", cloud_ids)
                            cursor.execute(f"DELETE FROM prisms WHERE id NOT IN ({placeholders})", cloud_ids)
                            removed = cursor.rowcount
                            conn.commit()
                            if removed > 0:
                                print(f"   ğŸ—‘ï¸ å·²ç§»é™¤ {removed} ä¸ªæœ¬åœ°å¤šä½™æ£±é•œï¼ˆä»¥äº‘ç«¯ä¸ºå‡†ï¼‰")
                        finally:
                            conn.close()
                    except Exception as e:
                        logger.warning(f"[PRISMS] æ¸…ç†æœ¬åœ°å¤šä½™æ£±é•œå¤±è´¥: {e}")

                    # å†™å› anchor_config æ—¶åªä¿ç•™äº‘ç«¯æ£±é•œï¼Œé¿å…æ—§æµ‹è¯•æ£±é•œä»æ˜¾ç¤º
                    anchor_config_to_save = {k: v for k, v in anchor_config_to_save.items() if k in cloud_ids}

                    # å°†äº‘ç«¯æ£±é•œçš„ is_active å†™å›æœ¬åœ° anchor_config_v2.jsonï¼ˆä½¿ç”¨ä¸è¯»å–æ—¶ç›¸åŒçš„è·¯å¾„ï¼‰
                    if anchor_config_to_save:
                        try:
                            write_path = anchor_config_path
                            if write_path is None:
                                from common import PathManager
                                pm = PathManager.get_instance()
                                write_path = pm.config_dir / "anchor_config_v2.json"
                            write_path.parent.mkdir(parents=True, exist_ok=True)
                            with open(write_path, 'w', encoding='utf-8') as f:
                                json.dump(anchor_config_to_save, f, ensure_ascii=False, indent=2)
                            logger.info(f"[PRISMS] å·²å†™å›æ£±é•œå¯ç”¨çŠ¶æ€åˆ° {write_path}")
                        except Exception as e:
                            logger.warning(f"[PRISMS] å†™å› anchor_config å¤±è´¥: {e}")
                else:
                    print("   âœ… æ— äº‘ç«¯æ£±é•œ")

            except Exception as e:
                error_msg = f"ä¸‹è½½äº‘ç«¯æ£±é•œå¤±è´¥: {e}"
                errors.append(error_msg)
                print(f"   âŒ {error_msg}")

            print()

            # æ€»ç»“
            print("=" * 60)
            print("âœ… æ£±é•œåŒæ­¥å®Œæˆ")
            print("=" * 60)
            print(f"ä¸Šä¼ : {uploaded}")
            print(f"ä¸‹è½½: {downloaded}")
            print(f"å†²çªè§£å†³: {conflicts_resolved}")
            if errors:
                print(f"é”™è¯¯: {len(errors)}")
                for error in errors:
                    print(f"   - {error}")

            return {
                'success': len(errors) == 0,
                'uploaded': uploaded,
                'downloaded': downloaded,
                'conflicts_resolved': conflicts_resolved,
                'errors': errors
            }

        except Exception as e:
            error_msg = f"æ£±é•œåŒæ­¥å¤±è´¥: {e}"
            errors.append(error_msg)
            print(f"âŒ {error_msg}")

            return {
                'success': False,
                'uploaded': uploaded,
                'downloaded': downloaded,
                'conflicts_resolved': conflicts_resolved,
                'errors': errors
            }


# ä¾¿æ·å‡½æ•°
def get_sync_service(db_path: str = None, api_url: str = None) -> SyncService:
    """
    è·å–åŒæ­¥æœåŠ¡å®ä¾‹

    Args:
        db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™ä» PathManager è·å–ï¼‰
        api_url: äº‘ç«¯ API URLï¼ˆå¯é€‰ï¼‰

    Returns:
        SyncService å®ä¾‹
    """
    if db_path is None:
        # ä»è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        from common import PathManager
        pm = PathManager.get_instance()
        db_path = pm.db_path

    return SyncService(str(db_path), api_url)
