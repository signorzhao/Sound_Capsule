import pandas as pd
import numpy as np
import os
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def refined_categorize():
    print("ğŸš€ å¯åŠ¨é«˜çº§è¯­ä¹‰è¯æ€§åˆ†ç±»å™¨...")
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # 1. å¼ºåŒ–çš„åŸå‹è¯åº“
    prototypes = {
        "noun": [
            "a place", "a location", "a building", "an object", "a material", "an entity",
            "cathedral", "lobby", "room", "hall", "studio", "forest", "engine", "metal",
            "water", "stone", "glass", "instrument", "machine", "animal", "creature",
            "texture", "timbre", "source", "materiality", "reverb", "space", "environment"
        ],
        "verb": [
            "to act", "to do", "an action", "a movement", "the process of",
            "hitting", "breaking", "moving", "sliding", "exploding", "vibrating",
            "crashing", "flowing", "acting", "striking", "bursting", "splitting"
        ],
        "adjective": [
            "a quality", "a characteristic", "a description", "a feeling", "a state",
            "dark", "bright", "soft", "hard", "gritty", "smooth", "ethereal", "ominous",
            "scary", "peaceful", "tense", "relaxed", "digital", "analog", "metallic",
            "wooden", "organic", "synthetic", "noisy", "quiet"
        ]
    }

    proto_embs = {cat: model.encode(words) for cat, words in prototypes.items()}
    
    # 2. è§„åˆ™å¼•æ“ (ä¼˜å…ˆçº§æœ€é«˜)
    def rule_based_check(row):
        en = str(row.get('word_en', '')).lower()
        cn = str(row.get('word_cn', '')).lower()
        hint = str(row.get('semantic_hint', '')).lower()
        
        # å¸¸è§åè¯åç¼€
        noun_indicators = ['room', 'hall', 'space', 'lobby', 'cathedral', 'building', 'studio', 'place', 'field', 'area', 'chamber', 'station', 'park', 'forest', 'cave', 'tunnel']
        # åœ¨è¿™é‡Œå¢åŠ â€œæ··å“â€è¿™ç±»ä¸“ä¸šåè¯
        noun_keys = ['reverb', 'echo', 'delay', 'ambience', 'drone', 'pad', 'sub']
        
        # åªè¦æè¿°ä¸­åŒ…å«è¿™äº›è¯ï¼Œå¤§æ¦‚ç‡æ˜¯åè¯æ€§æè¿°
        combined = f"{en} {cn} {hint}"
        
        if any(x in combined for x in noun_indicators): return 'noun'
        if any(x in en for x in noun_keys): return 'noun'
        
        # å¸¸è§å½¢å®¹è¯åç¼€ (æ³¨æ„ä¼˜å…ˆçº§)
        if en.endswith('less') or en.endswith('ous') or en.endswith('ish') or en.endswith('ery'):
            return 'adjective'
            
        return None

    # 3. æ‰§è¡Œåˆ†ç±»
    master_path = 'master_lexicon_v3.csv'
    df = pd.read_csv(master_path)
    
    print(f"æ­£åœ¨åˆ†æ {len(df)} ä¸ªè¯æ±‡...")
    word_list = df['word_en'].astype(str).tolist()
    word_embs = model.encode(word_list)

    final_cats = []
    for i, row in df.iterrows():
        # A. è§„åˆ™ä¼˜å…ˆ
        rule_cat = rule_based_check(row)
        if rule_cat:
            final_cats.append(rule_cat)
            continue
            
        # B. è¯­ä¹‰æŠ•ç¥¨
        # è®¡ç®—ä¸æ¯ä¸ªåŸå‹ç»„çš„å¹³å‡ç›¸ä¼¼åº¦
        scores = {}
        for cat in ['noun', 'verb', 'adjective']:
            sim = cosine_similarity(word_embs[i].reshape(1, -1), proto_embs[cat])[0]
            scores[cat] = np.max(sim) # å–æœ€å¤§ç›¸ä¼¼åº¦ä½œä¸ºå‚è€ƒ

        # é’ˆå¯¹éŸ³æ•ˆåº“çš„å¾®è°ƒï¼šå¦‚æœæ˜¯ Living (èµ·å±…å®¤)ï¼Œè™½ç„¶ Living è¯­ä¹‰ååŠ¨è¯ï¼Œä½†æœ€å¤§ç›¸ä¼¼åº¦å¯èƒ½è¢« Noun ç»„çš„ place å¸å¼•
        best_cat = max(scores, key=scores.get)
        final_cats.append(best_cat)

    df['category'] = final_cats

    # 4. æœ€åå…œåº•ï¼šé’ˆå¯¹ç”¨æˆ·æåˆ°çš„ç‰¹å®šè¯åº“ç»“æ„ä¿®æ­£
    # æ¯”å¦‚ï¼šè¯åº“é‡Œå¾ˆå¤š "Living" å…¶å®å¯¹åº”çš„æ˜¯ "Living Room"
    df.to_csv(master_path, index=False, encoding='utf-8-sig')
    print("âœ¨ åˆ†ç±»å™¨ä¼˜åŒ–å®Œæˆï¼Œå·²æ›´æ–°æ€»è¯åº“ã€‚")

if __name__ == "__main__":
    refined_categorize()
